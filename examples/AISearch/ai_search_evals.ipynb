{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Search Evals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide demonstrates how we developed Braintrust's AI-powered search bar, harnessing the power of Braintrust's evaluation workflow along the way. If you've used Braintrust before, you may be familiar with the project page, which serves as a home base for collections of eval experiments:\n",
    "\n",
    "![Braintrust Project Page](./assets/project-page-sql.png)\n",
    "\n",
    "To find a particular experiment, you can type filter and sort queries into the search bar, using standard SQL syntax. But SQL can be finicky -- it's very easy to run into syntax errors like single quotes instead of double, incorrect JSON extraction syntax, or typos. Users would prefer to just type in an intuitive search like `experiments run on git commit 2a43fd1` or `score under 0.5` and see a corresponding SQL query appear automatically. Let's achieve this using AI, with assistance from Braintrust's eval framework.\n",
    "\n",
    "We'll start by installing some packages and setting up our OpenAI client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (0.23.0)\n",
      "Collecting Levenshtein\n",
      "  Downloading Levenshtein-0.25.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: autoevals in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (0.0.46)\n",
      "Requirement already satisfied: braintrust in /Users/ankur/projects/braintrust/sdk/py/src (0.0.98)\n",
      "Requirement already satisfied: chevron in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: duckdb in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (0.8.1)\n",
      "Collecting duckdb\n",
      "  Downloading duckdb-0.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (763 bytes)\n",
      "Requirement already satisfied: openai in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (1.3.7)\n",
      "Collecting openai\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pydantic in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (2.4.2)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from Levenshtein) (3.4.0)\n",
      "Requirement already satisfied: pyyaml in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from autoevals) (6.0.1)\n",
      "Requirement already satisfied: braintrust-core in /Users/ankur/projects/braintrust/sdk/core/py/src (from autoevals) (0.0.16)\n",
      "Requirement already satisfied: GitPython in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from braintrust) (3.1.41)\n",
      "Requirement already satisfied: requests in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from braintrust) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from braintrust) (4.66.1)\n",
      "Requirement already satisfied: exceptiongroup==1.2.0 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from braintrust) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: sniffio in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic)\n",
      "  Using cached pydantic_core-2.16.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (0.16.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from GitPython->braintrust) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from requests->braintrust) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from requests->braintrust) (1.26.18)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython->braintrust) (5.0.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ankur/projects/braintrust/venv/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Downloading Levenshtein-0.25.0-cp311-cp311-macosx_11_0_arm64.whl (103 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading duckdb-0.10.0-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "Using cached pydantic_core-2.16.2-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: typing-extensions, Levenshtein, duckdb, pydantic-core, pydantic, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.3\n",
      "    Uninstalling typing_extensions-4.6.3:\n",
      "      Successfully uninstalled typing_extensions-4.6.3\n",
      "  Attempting uninstall: Levenshtein\n",
      "    Found existing installation: Levenshtein 0.23.0\n",
      "    Uninstalling Levenshtein-0.23.0:\n",
      "      Successfully uninstalled Levenshtein-0.23.0\n",
      "  Attempting uninstall: duckdb\n",
      "    Found existing installation: duckdb 0.8.1\n",
      "    Uninstalling duckdb-0.8.1:\n",
      "      Successfully uninstalled duckdb-0.8.1\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.10.1\n",
      "    Uninstalling pydantic_core-2.10.1:\n",
      "      Successfully uninstalled pydantic_core-2.10.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.4.2\n",
      "    Uninstalling pydantic-2.4.2:\n",
      "      Successfully uninstalled pydantic-2.4.2\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.3.7\n",
      "    Uninstalling openai-1.3.7:\n",
      "      Successfully uninstalled openai-1.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "databind-json 4.4.1 requires typing-extensions<4.7,>=3.10.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
      "chalice 1.30.0 requires pip<23.4,>=9, but you have pip 24.0 which is incompatible.\n",
      "databind-core 4.4.1 requires typing-extensions<4.7,>=3.10.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Levenshtein-0.25.0 duckdb-0.10.0 openai-1.12.0 pydantic-2.6.1 pydantic-core-2.16.2 typing-extensions-4.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U Levenshtein autoevals braintrust chevron duckdb openai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import braintrust\n",
    "import openai\n",
    "\n",
    "PROJECT_NAME = \"AI Search Cookbook\"\n",
    "\n",
    "# We use the Braintrust proxy here to get access to caching, but this is totally optional!\n",
    "openai_opts = dict(\n",
    "    base_url=\"https://braintrustproxy.com/v1\",\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\"),\n",
    ")\n",
    "client = braintrust.wrap_openai(openai.AsyncOpenAI(default_headers={\"x-bt-use-cache\": \"always\"}, **openai_opts))\n",
    "\n",
    "braintrust.login(api_key=os.environ.get(\"BRAINTRUST_API_KEY\", \"YOUR_BRAINRUST_API_KEY\"))\n",
    "dataset = braintrust.init_dataset(PROJECT_NAME, \"AI Search Cookbook\", use_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and render the templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ask GPT to translate a search query, we have to account for multiple output options: (1) a SQL filter, (2) a SQL sort, (3) both of the above, or (4) an unsuccessful translation (e.g. for a nonsensical user input). We'll use [function calling](https://platform.openai.com/docs/guides/function-calling) to robustly handle each distinct scenario, with the following output format:\n",
    "\n",
    "- `match`: Whether or not the model was able to translate the search into a valid SQL filter/sort.\n",
    "- `filter`: A `WHERE` clause.\n",
    "- `sort`: An `ORDER BY` clause.\n",
    "- `explanation`: Explanation for the choices above -- this is useful for debugging and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import Literal, Optional, Union\n",
    "\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class FunctionCallOutput:\n",
    "    match: Optional[bool] = None\n",
    "    filter: Optional[str] = None\n",
    "    sort: Optional[str] = None\n",
    "    explanation: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class Match(BaseModel):\n",
    "    type: Literal[\"MATCH\"] = \"MATCH\"\n",
    "    explanation: str = Field(\n",
    "        ..., description=\"Explanation of why I called the MATCH function\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SQL(BaseModel):\n",
    "    type: Literal[\"SQL\"] = \"SQL\"\n",
    "    filter: Optional[str] = Field(..., description=\"SQL filter clause\")\n",
    "    sort: Optional[str] = Field(..., description=\"SQL sort clause\")\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=\"Explanation of why I called the SQL function and how I chose the filter and/or sort clauses\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    value: Union[Match, SQL] = Field(\n",
    "        ...,\n",
    "    )\n",
    "\n",
    "\n",
    "def function_choices():\n",
    "    return [\n",
    "        {\n",
    "            \"name\": \"QUERY\",\n",
    "            \"description\": \"Break down the query either into a MATCH or SQL call\",\n",
    "            \"parameters\": Query.model_json_schema(),\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompts for evaluation in Braintrust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate two different prompts: a shorter prompt with a brief explanation of the problem statement and description of the experiment schema, and a longer prompt that additionally contains a feed of example cases to guide the model. There's nothing special about either of these prompts, and that's OK -- we can iterate and improve the prompts when we use Braintrust to drill down into the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SHORT_PROMPT_FILE = \"./assets/short_prompt.tmpl\"\n",
    "LONG_PROMPT_FILE = \"./assets/long_prompt.tmpl\"\n",
    "FEW_SHOT_EXAMPLES_FILE = \"./assets/few_shot.json\"\n",
    "\n",
    "with open(SHORT_PROMPT_FILE) as f:\n",
    "    short_prompt = f.read()\n",
    "\n",
    "with open(LONG_PROMPT_FILE) as f:\n",
    "    long_prompt = f.read()\n",
    "\n",
    "with open(FEW_SHOT_EXAMPLES_FILE, \"r\") as f:\n",
    "    few_shot_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail worth mentioning: each prompt contains a stub for dynamic insertion of the data schema. This is motivated by the need to handle semantic searches like `more than 40 examples` or `score < 0.5` that don't directly reference a column in the base table. We need to tell the model how the data is structured and what each fields actually _means_. We'll construct a descriptive schema using [pydantic](link) and paste it into each prompt to provide the model with this information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "import chevron\n",
    "\n",
    "\n",
    "class ExperimentGitState(BaseModel):\n",
    "    commit: str = Field(\n",
    "        ...,\n",
    "        description=\"Git commit hash. Any prefix of this hash at least 7 characters long should be considered an exact match, so use a substring filter rather than string equality to check the commit, e.g. `(source->>'commit') ILIKE '{COMMIT}%'`\",\n",
    "    )\n",
    "    branch: str = Field(..., description=\"Git branch name\")\n",
    "    tag: Optional[str] = Field(..., description=\"Git commit tag\")\n",
    "    commit_time: int = Field(..., description=\"Git commit timestamp\")\n",
    "    author_name: str = Field(..., description=\"Author of git commit\")\n",
    "    author_email: str = Field(..., description=\"Email address of git commit author\")\n",
    "    commit_message: str = Field(..., description=\"Git commit message\")\n",
    "    dirty: Optional[bool] = Field(\n",
    "        ...,\n",
    "        description=\"Whether the git state was dirty when the experiment was run. If false, the git state was clean\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Experiment(BaseModel):\n",
    "    id: str = Field(..., description=\"Experiment ID, unique\")\n",
    "    name: str = Field(..., description=\"Name of the experiment\")\n",
    "    last_updated: int = Field(\n",
    "        ...,\n",
    "        description=\"Timestamp marking when the experiment was last updated. If the query deals with some notion of relative time, like age or recency, refer to this timestamp and, if appropriate, compare it to the current time `get_current_time()` by adding or subtracting an interval.\",\n",
    "    )\n",
    "    creator: Dict[str, str] = Field(..., description=\"Information about the experiment creator\")\n",
    "    source: ExperimentGitState = Field(..., description=\"Git state that the experiment was run on\")\n",
    "    metadata: Dict[str, Any] = Field(\n",
    "        ...,\n",
    "        description=\"Custom metadata provided by the user. Ignore this field unless the query mentions metadata or refers to a metadata key specifically\",\n",
    "    )\n",
    "\n",
    "\n",
    "def build_experiment_schema(score_fields: List[str]):\n",
    "    ExperimentWithScoreFields = create_model(\n",
    "        \"Experiment\",\n",
    "        __base__=Experiment,\n",
    "        **{field: (Optional[float], ...) for field in score_fields},\n",
    "    )\n",
    "    return json.dumps(ExperimentWithScoreFields.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompts are ready! Before we run our evals, we just need to load some sample data and define our scoring functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our examples. Each example case contains `input` (the search query) and `expected` (function call output).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Example:\n",
    "    input: str\n",
    "    expected: FunctionCallOutput\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "EXAMPLES_FILE = \"./assets/examples.json\"\n",
    "with open(EXAMPLES_FILE) as f:\n",
    "    examples_json = json.load(f)\n",
    "\n",
    "templates = [\n",
    "    Example(input=e[\"input\"], expected=FunctionCallOutput(**e[\"expected\"])) for e in examples_json[\"examples\"]\n",
    "]\n",
    "\n",
    "# Each example contains a few dynamic fields that depends on the experiments\n",
    "# we're searching over. For simplicity, we'll hard-code these fields here.\n",
    "SCORE_FIELDS = [\"avg_sql_score\", \"avg_factuality_score\"]\n",
    "\n",
    "\n",
    "def render_example(example: Example, args: Dict[str, Any]) -> Example:\n",
    "    render_optional = lambda template: (chevron.render(template, args, warn=True) if template is not None else None)\n",
    "    return Example(\n",
    "        input=render_optional(example.input),\n",
    "        expected=FunctionCallOutput(\n",
    "            match=example.expected.match,\n",
    "            filter=render_optional(example.expected.filter),\n",
    "            sort=render_optional(example.expected.sort),\n",
    "            explanation=render_optional(example.expected.explanation),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "examples = [render_example(t, {\"score_fields\": SCORE_FIELDS}) for t in templates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also split the examples into a training set and test set. For now, this won't matter, but later on when we fine-tune the model, we'll want to use the test set to evaluate the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(examples):\n",
    "    if i < 0.8 * len(examples):\n",
    "        e.metadata = {\"split\": \"train\"}\n",
    "    else:\n",
    "        e.metadata = {\"split\": \"test\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert our examples into a Braintrust dataset so we can introspect and reuse the data later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 45 records. Here are the first 2...\n",
      "{'id': '0d864030-959e-448f-9ad4-be0093a70f55', 'span_id': '2a4e36bb-2dcd-4492-83cf-83d41d896bf1', 'root_span_id': '2a4e36bb-2dcd-4492-83cf-83d41d896bf1', '_xact_id': 1000192554042763293, 'created': '2024-02-19T22:55:33.819976Z', 'project_id': 'd90ca11f-a908-4302-8674-5f17fff8f343', 'dataset_id': 'f0ce1b0a-831c-4bb4-98db-92d73a547c0c', 'input': 'asdf', 'expected': {'sort': None, 'error': None, 'match': True, 'filter': None, 'explanation': 'The query has no semantic meaning, so I interpret it as a simple substring match and fall back to the MATCH function.'}, 'metadata': {'split': 'train'}}\n",
      "{'id': '14fdfe0e-c723-40f3-8667-8832143433d1', 'span_id': 'c0516bad-0989-46e7-8f13-8447dcdc4bc4', 'root_span_id': 'c0516bad-0989-46e7-8f13-8447dcdc4bc4', '_xact_id': 1000192554042763294, 'created': '2024-02-19T22:55:33.821139Z', 'project_id': 'd90ca11f-a908-4302-8674-5f17fff8f343', 'dataset_id': 'f0ce1b0a-831c-4bb4-98db-92d73a547c0c', 'input': 'metadata.env.veggies includes kale or spinach', 'expected': {'sort': None, 'error': None, 'match': False, 'filter': \"(metadata->'env'->>'veggies') ILIKE '%kale%' OR (metadata->'env'->>'veggies') ILIKE '%spinach%'\", 'explanation': None}, 'metadata': {'split': 'train'}}\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    # dataset.insert(**dataclasses.asdict(example))\n",
    "    dataset.insert(\n",
    "        input=example.input, expected=example.expected, metadata=example.metadata\n",
    "    )\n",
    "dataset.flush()\n",
    "\n",
    "records = list(dataset)\n",
    "print(f\"Generated {len(records)} records. Here are the first 2...\")\n",
    "for record in records[:2]:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define scoring functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we score our outputs against the ground truth queries? We can't rely on an exact text match, since there are multiple correct ways to translate a SQL query. Instead, we'll use two approximate scoring methods: (1) `SQLScorer`, which roundtrips each query through `json_serialize_sql` to normalize before attempting a direct comparison, and (2) `AutoScorer`, which delegates the scoring task to `gpt-4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from braintrust import current_span, traced\n",
    "from Levenshtein import distance\n",
    "\n",
    "from autoevals import Score, Scorer, Sql\n",
    "\n",
    "EXPERIMENTS_TABLE = \"./assets/experiments.parquet\"\n",
    "SUMMARY_TABLE = \"./assets/experiments_summary.parquet\"\n",
    "duckdb.sql(f\"DROP TABLE IF EXISTS experiments; CREATE TABLE experiments AS SELECT * FROM '{EXPERIMENTS_TABLE}'\")\n",
    "duckdb.sql(\n",
    "    f\"DROP TABLE IF EXISTS experiments_summary; CREATE TABLE experiments_summary AS SELECT * FROM '{SUMMARY_TABLE}'\"\n",
    ")\n",
    "\n",
    "\n",
    "def _test_clause(*, filter=None, sort=None) -> bool:\n",
    "    clause = f\"\"\"\n",
    "        SELECT\n",
    "          experiments.id AS id,\n",
    "          experiments.name,\n",
    "          experiments_summary.last_updated,\n",
    "          experiments.user AS creator,\n",
    "          experiments.repo_info AS source,\n",
    "          experiments_summary.* EXCLUDE (experiment_id, last_updated),\n",
    "        FROM experiments\n",
    "        LEFT JOIN experiments_summary ON experiments.id = experiments_summary.experiment_id\n",
    "        {'WHERE ' + filter if filter else ''}\n",
    "        {'ORDER BY ' + sort if sort else ''}\n",
    "    \"\"\"\n",
    "    current_span().log(metadata=dict(test_clause=clause))\n",
    "    try:\n",
    "        duckdb.sql(clause).fetchall()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _single_quote(s):\n",
    "    return f\"\"\"'{s.replace(\"'\", \"''\")}'\"\"\"\n",
    "\n",
    "\n",
    "def _roundtrip_filter(s):\n",
    "    return duckdb.sql(\n",
    "        f\"\"\"\n",
    "        SELECT json_deserialize_sql(json_serialize_sql({_single_quote(f\"SELECT 1 WHERE {s}\")}))\n",
    "    \"\"\"\n",
    "    ).fetchall()[0][0]\n",
    "\n",
    "\n",
    "def _roundtrip_sort(s):\n",
    "    return duckdb.sql(\n",
    "        f\"\"\"\n",
    "        SELECT json_deserialize_sql(json_serialize_sql({_single_quote(f\"SELECT 1 ORDER BY {s}\")}))\n",
    "    \"\"\"\n",
    "    ).fetchall()[0][0]\n",
    "\n",
    "\n",
    "def score_clause(\n",
    "    output: Optional[str],\n",
    "    expected: Optional[str],\n",
    "    roundtrip: Callable[[str], str],\n",
    "    test_clause: Callable[[str], bool],\n",
    ") -> float:\n",
    "    exact_match = 1 if output == expected else 0\n",
    "    current_span().log(scores=dict(exact_match=exact_match))\n",
    "    if exact_match:\n",
    "        return 1\n",
    "\n",
    "    roundtrip_match = 0\n",
    "    try:\n",
    "        if roundtrip(output) == roundtrip(expected):\n",
    "            roundtrip_match = 1\n",
    "    except Exception as e:\n",
    "        current_span.log(metadata=dict(roundtrip_error=str(e)))\n",
    "        \n",
    "    current_span().log(scores=dict(roundtrip_match=roundtrip_match))\n",
    "    if roundtrip_match:\n",
    "        return 1\n",
    "\n",
    "    # If the queries aren't equivalent after roundtripping, it's not immediately clear\n",
    "    # whether they are semantically equivalent. Let's at least check that the generated\n",
    "    # clause is valid SQL by running the `test_clause` function defined above, which\n",
    "    # runs a test query against our sample data.\n",
    "    valid_clause_score = 1 if test_clause(output) else 0\n",
    "    current_span().log(scores=dict(valid_clause=valid_clause_score))\n",
    "    if valid_clause_score == 0:\n",
    "        return 0\n",
    "\n",
    "    max_len = max(len(clause) for clause in [output, expected])\n",
    "    if max_len == 0:\n",
    "        current_span().log(metadata=dict(error=\"Bad example: empty clause\"))\n",
    "        return 0\n",
    "    return 1 - (distance(output, expected) / max_len)\n",
    "\n",
    "\n",
    "class SQLScorer(Scorer):\n",
    "    \"\"\"SQLScorer uses DuckDB's `json_serialize_sql` function to determine whether\n",
    "    the model's chosen filter/sort clause(s) are equivalent to the expected\n",
    "    outputs. If not, we assign partial credit to each clause depending on\n",
    "    (1) whether the clause is valid SQL, as determined by running it against\n",
    "    the actual data and seeing if it errors, and (2) a distance-wise comparison\n",
    "    to the expected text.\n",
    "    \"\"\"\n",
    "\n",
    "    def _run_eval_sync(\n",
    "        self,\n",
    "        output,\n",
    "        expected=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"SQLScorer requires an expected value\")\n",
    "\n",
    "        name = \"SQLScorer\"\n",
    "        expected = FunctionCallOutput(**expected)\n",
    "\n",
    "        function_choice_score = 1 if output.match == expected.match else 0\n",
    "        current_span().log(scores=dict(function_choice=function_choice_score))\n",
    "        if function_choice_score == 0:\n",
    "            return Score(name=name, score=0)\n",
    "        if expected.match:\n",
    "            return Score(name=name, score=1)\n",
    "\n",
    "        filter_score = None\n",
    "        if output.filter and expected.filter:\n",
    "            with current_span().start_span(\"SimpleFilter\") as span:\n",
    "                filter_score = score_clause(\n",
    "                    output.filter,\n",
    "                    expected.filter,\n",
    "                    _roundtrip_filter,\n",
    "                    lambda s: _test_clause(filter=s),\n",
    "                )\n",
    "        elif output.filter or expected.filter:\n",
    "            filter_score = 0\n",
    "        current_span().log(scores=dict(filter=filter_score))\n",
    "\n",
    "        sort_score = None\n",
    "        if output.sort and expected.sort:\n",
    "            with current_span().start_span(\"SimpleSort\") as span:\n",
    "                sort_score = score_clause(\n",
    "                    output.sort,\n",
    "                    expected.sort,\n",
    "                    _roundtrip_sort,\n",
    "                    lambda s: _test_clause(sort=s),\n",
    "                )\n",
    "        elif output.sort or expected.sort:\n",
    "            sort_score = 0\n",
    "        current_span().log(scores=dict(sort=sort_score))\n",
    "\n",
    "        scores = [s for s in [filter_score, sort_score] if s is not None]\n",
    "        if len(scores) == 0:\n",
    "            return Score(\n",
    "                name=name,\n",
    "                score=0,\n",
    "                error=\"Bad example: no filter or sort for SQL function call\",\n",
    "            )\n",
    "        return Score(name=name, score=sum(scores) / len(scores))\n",
    "\n",
    "\n",
    "@traced(\"auto_score_filter\")\n",
    "def auto_score_filter(openai_opts, **kwargs):\n",
    "    return Sql(**openai_opts)(**kwargs)\n",
    "\n",
    "\n",
    "@traced(\"auto_score_sort\")\n",
    "def auto_score_sort(openai_opts, **kwargs):\n",
    "    return Sql(**openai_opts)(**kwargs)\n",
    "\n",
    "\n",
    "class AutoScorer(Scorer):\n",
    "    \"\"\"AutoScorer uses the `Sql` scorer from the autoevals library to auto-score\n",
    "    the model's chosen filter/sort clause(s) against the expected outputs\n",
    "    using an LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **openai_opts):\n",
    "        self.openai_opts = openai_opts\n",
    "\n",
    "    def _run_eval_sync(\n",
    "        self,\n",
    "        output,\n",
    "        expected=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"AutoScorer requires an expected value\")\n",
    "        input = kwargs.get(\"input\")\n",
    "        if input is None or not isinstance(input, str):\n",
    "            raise ValueError(\"AutoScorer requires an input value of type str\")\n",
    "\n",
    "        name = \"AutoScorer\"\n",
    "        expected = FunctionCallOutput(**expected)\n",
    "\n",
    "        function_choice_score = 1 if output.match == expected.match else 0\n",
    "        current_span().log(scores=dict(function_choice=function_choice_score))\n",
    "        if function_choice_score == 0:\n",
    "            return Score(name=name, score=0)\n",
    "        if expected.match:\n",
    "            return Score(name=name, score=1)\n",
    "\n",
    "        filter_score = None\n",
    "        if output.filter and expected.filter:\n",
    "            result = auto_score_filter(\n",
    "                openai_opts=self.openai_opts,\n",
    "                input=input,\n",
    "                output=output.filter,\n",
    "                expected=expected.filter,\n",
    "            )\n",
    "            filter_score = result.score or 0\n",
    "        elif output.filter or expected.filter:\n",
    "            filter_score = 0\n",
    "        current_span().log(scores=dict(filter=filter_score))\n",
    "\n",
    "        sort_score = None\n",
    "        if output.sort and expected.sort:\n",
    "            result = auto_score_sort(\n",
    "                openai_opts=self.openai_opts,\n",
    "                input=input,\n",
    "                output=output.sort,\n",
    "                expected=expected.sort,\n",
    "            )\n",
    "            sort_score = result.score or 0\n",
    "        elif output.sort or expected.sort:\n",
    "            sort_score = 0\n",
    "        current_span().log(scores=dict(sort=sort_score))\n",
    "\n",
    "        scores = [s for s in [filter_score, sort_score] if s is not None]\n",
    "        if len(scores) == 0:\n",
    "            return Score(\n",
    "                name=name,\n",
    "                score=0,\n",
    "                error=\"Bad example: no filter or sort for SQL function call\",\n",
    "            )\n",
    "        return Score(name=name, score=sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evals!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Braintrust `Eval` framework to set up our experiments according to the prompts, dataset, and scoring functions defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_completion_kwargs(\n",
    "    *,\n",
    "    query: str,\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    score_fields: List[str],\n",
    "    **kwargs,\n",
    "):\n",
    "    # Inject the JSON schema into the prompt to assist the model.\n",
    "    schema = build_experiment_schema(score_fields=score_fields)\n",
    "    system_message = chevron.render(\n",
    "        prompt.strip(), {\"schema\": schema, \"examples\": few_shot_examples}, warn=True\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {query}\"},\n",
    "    ]\n",
    "\n",
    "    # We use the legacy function choices format for now, because fine-tuning still requires it.\n",
    "    return dict(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=messages,\n",
    "        functions=function_choices(),\n",
    "        function_call={\"name\": \"QUERY\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def format_output(completion):\n",
    "    try:\n",
    "        function_call = completion.choices[0].message.function_call\n",
    "        arguments = json.loads(function_call.arguments)[\"value\"]\n",
    "        match = arguments.pop(\"type\").lower() == \"match\"\n",
    "        return FunctionCallOutput(match=match, **arguments)\n",
    "    except Exception as e:\n",
    "        return FunctionCallOutput(error=str(e))\n",
    "\n",
    "\n",
    "GRADER = \"gpt-4\"  # Used by AutoScorer to grade the model outputs\n",
    "\n",
    "\n",
    "def make_task(model, prompt, score_fields):\n",
    "    async def task(input):\n",
    "        completion_kwargs = build_completion_kwargs(\n",
    "            query=input,\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            score_fields=score_fields,\n",
    "        )\n",
    "        return format_output(await client.chat.completions.create(**completion_kwargs))\n",
    "\n",
    "    return task\n",
    "\n",
    "\n",
    "async def run_eval(experiment_name, prompt, model, score_fields=SCORE_FIELDS):\n",
    "    task = make_task(model, prompt, score_fields)\n",
    "    await braintrust.Eval(\n",
    "        name=PROJECT_NAME,\n",
    "        experiment_name=experiment_name,\n",
    "        data=dataset,\n",
    "        task=task,\n",
    "        scores=[SQLScorer(), AutoScorer(**openai_opts, model=GRADER)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on one example before running an eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCallOutput(match=True, filter=None, sort=None, explanation='The query is unclear and does not have semantic meaning, falling back to a basic substring match.', error=None)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = build_completion_kwargs(\n",
    "    query=list(dataset)[0][\"input\"],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=short_prompt,\n",
    "    score_fields=SCORE_FIELDS,\n",
    ")\n",
    "response = await client.chat.completions.create(**args)\n",
    "format_output(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it! We'll use `gpt-3.5-turbo` for both evals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Short Prompt 2.0-0d8bbbab is running at http://localhost:3000/app/braintrustdata.com/p/AI%20Search%20Cookbook/Short%20Prompt%202.0-0d8bbbab\n",
      "AI Search Cookbook [experiment_name=Short Prompt 2.0] (data): 45it [00:00, 148617.07it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ff733fbea7475690249f490027ef77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AI Search Cookbook [experiment_name=Short Prompt 2.0] (tasks):   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Short Prompt 2.0-0d8bbbab compared to Fine tuned model:\n",
      "41.11% (+05.56%) 'AutoScorer'      score\t(13 improvements, 11 regressions)\n",
      "21.43% (-03.57%) 'exact_match'     score\t(2 improvements, 2 regressions)\n",
      "62.22% (-11.11%) 'function_choice' score\t(8 improvements, 13 regressions)\n",
      "11.76% (-03.05%) 'roundtrip_match' score\t(1 improvements, 2 regressions)\n",
      "80.00% (+19.13%) 'valid_clause'    score\t(4 improvements, 0 regressions)\n",
      "61.92% (+10.06%) 'sort'            score\t(1 improvements, 2 regressions)\n",
      "63.86% (+10.01%) 'filter'          score\t(8 improvements, 9 regressions)\n",
      "50.07% (+08.27%) 'SQLScorer'       score\t(16 improvements, 17 regressions)\n",
      "\n",
      "38.15s (-2705.72%) 'duration'\t(32 improvements, 13 regressions)\n",
      "\n",
      "See results for Short Prompt 2.0-0d8bbbab at http://localhost:3000/app/braintrustdata.com/p/AI%20Search%20Cookbook/Short%20Prompt%202.0-0d8bbbab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mEvaluator AI Search Cookbook [experiment_name=Short Prompt 2.0] failed with 2 errors\u001b[0m\n",
      "\u001b[91m  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 596, in run_evaluator_task\n",
      "  |     raise exceptiongroup.ExceptionGroup(\n",
      "  | ExceptionGroup: Found exceptions for the following scorers: SQLScorer (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 570, in run_evaluator_task\n",
      "    |     passing_scorers_and_results.append((name, await p))\n",
      "    |                                               ^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 516, in await_or_run_scorer\n",
      "    |     result = await await_or_run(score, **scorer_args)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 487, in await_or_run\n",
      "    |     return await f(*args, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 33, in eval_async\n",
      "    |     return await self._run_eval_async(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 43, in _run_eval_async\n",
      "    |     return self._run_eval_sync(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 122, in _run_eval_sync\n",
      "    |     filter_score = score_clause(\n",
      "    |                    ^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 70, in score_clause\n",
      "    |     roundtrip_match = 1 if roundtrip(output) == roundtrip(expected) else 0\n",
      "    |                                                 ^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 48, in _roundtrip_filter\n",
      "    |     ).fetchall()[0][0]\n",
      "    |       ^^^^^^^^^^\n",
      "    | duckdb.duckdb.ParserException: Parser Error: Error parsing json: parser: syntax error at or near \")\"\n",
      "    +------------------------------------\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 596, in run_evaluator_task\n",
      "  |     raise exceptiongroup.ExceptionGroup(\n",
      "  | ExceptionGroup: Found exceptions for the following scorers: SQLScorer (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 570, in run_evaluator_task\n",
      "    |     passing_scorers_and_results.append((name, await p))\n",
      "    |                                               ^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 516, in await_or_run_scorer\n",
      "    |     result = await await_or_run(score, **scorer_args)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 487, in await_or_run\n",
      "    |     return await f(*args, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 33, in eval_async\n",
      "    |     return await self._run_eval_async(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 43, in _run_eval_async\n",
      "    |     return self._run_eval_sync(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 135, in _run_eval_sync\n",
      "    |     sort_score = score_clause(\n",
      "    |                  ^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 70, in score_clause\n",
      "    |     roundtrip_match = 1 if roundtrip(output) == roundtrip(expected) else 0\n",
      "    |                            ^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 56, in _roundtrip_sort\n",
      "    |     ).fetchall()[0][0]\n",
      "    |       ^^^^^^^^^^\n",
      "    | duckdb.duckdb.ParserException: Parser Error: Error parsing json: parser: syntax error at or near \"DESC\"\n",
      "    +------------------------------------\u001b[0m\n",
      "\u001b[91mAdd --verbose to see full stack traces.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await run_eval(\"Short Prompt 2.0\", short_prompt, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Long Prompt 2.0-ec55ed88 is running at http://localhost:3000/app/braintrustdata.com/p/AI%20Search%20Cookbook/Long%20Prompt%202.0-ec55ed88\n",
      "AI Search Cookbook [experiment_name=Long Prompt 2.0] (data): 45it [00:00, 195589.31it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd63855657544ef8ee5e4362025807c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AI Search Cookbook [experiment_name=Long Prompt 2.0] (tasks):   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Long Prompt 2.0-ec55ed88 compared to Short Prompt 2.0-0d8bbbab:\n",
      "54.44% (+13.33%) 'AutoScorer'      score\t(11 improvements, 5 regressions)\n",
      "50.00% (+28.57%) 'exact_match'     score\t(5 improvements, 1 regressions)\n",
      "77.78% (+15.56%) 'function_choice' score\t(9 improvements, 2 regressions)\n",
      "03.33% (-08.43%) 'roundtrip_match' score\t(1 improvements, 1 regressions)\n",
      "90.00% (+10.00%) 'valid_clause'    score\t(1 improvements, 1 regressions)\n",
      "75.48% (+13.56%) 'sort'            score\t(2 improvements, 0 regressions)\n",
      "67.15% (+03.29%) 'filter'          score\t(10 improvements, 3 regressions)\n",
      "67.61% (+17.54%) 'SQLScorer'       score\t(19 improvements, 4 regressions)\n",
      "\n",
      "28.37s (-978.02%) 'duration'\t(24 improvements, 21 regressions)\n",
      "\n",
      "See results for Long Prompt 2.0-ec55ed88 at http://localhost:3000/app/braintrustdata.com/p/AI%20Search%20Cookbook/Long%20Prompt%202.0-ec55ed88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mEvaluator AI Search Cookbook [experiment_name=Long Prompt 2.0] failed with 2 errors\u001b[0m\n",
      "\u001b[91m  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 596, in run_evaluator_task\n",
      "  |     raise exceptiongroup.ExceptionGroup(\n",
      "  | ExceptionGroup: Found exceptions for the following scorers: SQLScorer (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 570, in run_evaluator_task\n",
      "    |     passing_scorers_and_results.append((name, await p))\n",
      "    |                                               ^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 516, in await_or_run_scorer\n",
      "    |     result = await await_or_run(score, **scorer_args)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 487, in await_or_run\n",
      "    |     return await f(*args, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 33, in eval_async\n",
      "    |     return await self._run_eval_async(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 43, in _run_eval_async\n",
      "    |     return self._run_eval_sync(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 122, in _run_eval_sync\n",
      "    |     filter_score = score_clause(\n",
      "    |                    ^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 70, in score_clause\n",
      "    |     roundtrip_match = 1 if roundtrip(output) == roundtrip(expected) else 0\n",
      "    |                                                 ^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 48, in _roundtrip_filter\n",
      "    |     ).fetchall()[0][0]\n",
      "    |       ^^^^^^^^^^\n",
      "    | duckdb.duckdb.ParserException: Parser Error: Error parsing json: parser: syntax error at or near \")\"\n",
      "    +------------------------------------\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 596, in run_evaluator_task\n",
      "  |     raise exceptiongroup.ExceptionGroup(\n",
      "  | ExceptionGroup: Found exceptions for the following scorers: SQLScorer (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 570, in run_evaluator_task\n",
      "    |     passing_scorers_and_results.append((name, await p))\n",
      "    |                                               ^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 516, in await_or_run_scorer\n",
      "    |     result = await await_or_run(score, **scorer_args)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 487, in await_or_run\n",
      "    |     return await f(*args, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 33, in eval_async\n",
      "    |     return await self._run_eval_async(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 43, in _run_eval_async\n",
      "    |     return self._run_eval_sync(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 122, in _run_eval_sync\n",
      "    |     filter_score = score_clause(\n",
      "    |                    ^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 70, in score_clause\n",
      "    |     roundtrip_match = 1 if roundtrip(output) == roundtrip(expected) else 0\n",
      "    |                            ^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 48, in _roundtrip_filter\n",
      "    |     ).fetchall()[0][0]\n",
      "    |       ^^^^^^^^^^\n",
      "    | duckdb.duckdb.ParserException: Parser Error: Error parsing json: parser: syntax error at or near \"ASC\"\n",
      "    +------------------------------------\u001b[0m\n",
      "\u001b[91mAdd --verbose to see full stack traces.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await run_eval(\"Long Prompt 2.0\", long_prompt, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results in Braintrust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evals will generate a link to the experiment page. Click into an experiment to view the results!\n",
    "\n",
    "If you've just been following along, you can [check out some sample results here](). Type some searches into the search bar to see AI search in action. :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Braintrust Project Page](./assets/project-page-sql.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Let's try to fine-tune the model with an exceedingly short prompt. We'll use the same dataset and scoring functions, but we'll change the prompt to be more concise. To start, let's play with one example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\n",
      "{\n",
      "  \"sort\": null,\n",
      "  \"error\": null,\n",
      "  \"match\": true,\n",
      "  \"filter\": null,\n",
      "  \"explanation\": \"The query has no semantic meaning, so I interpret it as a simple substring match and fall back to the MATCH function.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "first = list(dataset.fetch())[0]\n",
    "print(first[\"input\"])\n",
    "print(json.dumps(first[\"expected\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY\n",
      "{'value': {'explanation': 'The query does not have semantic meaning that can '\n",
      "                          'be related to the named columns and JSON fields in '\n",
      "                          'the provided schema, so I default to calling the '\n",
      "                          'MATCH function to be safe.',\n",
      "           'type': 'MATCH'}}\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "from pprint import pprint\n",
    "\n",
    "long_prompt_args = build_completion_kwargs(\n",
    "    query=first[\"input\"],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=long_prompt,\n",
    "    score_fields=SCORE_FIELDS,\n",
    ")\n",
    "output = await client.chat.completions.create(**long_prompt_args)\n",
    "function_call = output.choices[0].message.function_call\n",
    "print(function_call.name)\n",
    "pprint(json.loads(function_call.arguments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's turn the output from the dataset into the tool call format that [OpenAI expects](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'QUERY',\n",
       " 'arguments': '{\"value\": {\"type\": \"MATCH\", \"explanation\": \"The query has no semantic meaning, so I interpret it as a simple substring match and fall back to the MATCH function.\"}}'}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_function_call(expected_value):\n",
    "    return {\n",
    "        \"name\": \"QUERY\",\n",
    "        \"arguments\": json.dumps(\n",
    "            {\n",
    "                \"value\": {\n",
    "                    \"type\": (\n",
    "                        expected_value.get(\"function\")\n",
    "                        if expected_value.get(\"function\")\n",
    "                        else \"MATCH\" if expected_value.get(\"match\") else \"SQL\"\n",
    "                    ),\n",
    "                    **{\n",
    "                        k: v\n",
    "                        for (k, v) in expected_value.items()\n",
    "                        if k in (\"filter\", \"sort\", \"explanation\") and v is not None\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "transform_function_call(first[\"expected\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function also works on our few shot examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'QUERY',\n",
       " 'arguments': '{\"value\": {\"type\": \"SQL\", \"filter\": \"(metrics->>\\'accuracy\\')::NUMERIC < 0.2\", \"explanation\": \"The query refers to a JSON field, so I correct the JSON extraction syntax according to directive 4 and cast the result to NUMERIC to compare to the value `0.2` as per directive 9.\"}}'}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_function_call(few_shot_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're fine-tuning, we can also use a shorter prompt that just contains the object type (Experiment) and schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINE_TUNING_PROMPT_FILE = \"./assets/fine_tune.tmpl\"\n",
    "\n",
    "with open(FINE_TUNING_PROMPT_FILE) as f:\n",
    "    fine_tune_prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Table: experiments\\n\\n<Schema>\\n{\"$defs\": {\"ExperimentGitState\": {\"properties\": {\"commit\": {\"description\": \"Git commit hash. Any prefix of this hash at least 7 characters long should be considered an exact match, so use a substring filter rather than string equality to check the commit, e.g. `(source->>\\'commit\\') ILIKE \\'{COMMIT}%\\'`\", \"title\": \"Commit\", \"type\": \"string\"}, \"branch\": {\"description\": \"Git branch name\", \"title\": \"Branch\", \"type\": \"string\"}, \"tag\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"description\": \"Git commit tag\", \"title\": \"Tag\"}, \"commit_time\": {\"description\": \"Git commit timestamp\", \"title\": \"Commit Time\", \"type\": \"integer\"}, \"author_name\": {\"description\": \"Author of git commit\", \"title\": \"Author Name\", \"type\": \"string\"}, \"author_email\": {\"description\": \"Email address of git commit author\", \"title\": \"Author Email\", \"type\": \"string\"}, \"commit_message\": {\"description\": \"Git commit message\", \"title\": \"Commit Message\", \"type\": \"string\"}, \"dirty\": {\"anyOf\": [{\"type\": \"boolean\"}, {\"type\": \"null\"}], \"description\": \"Whether the git state was dirty when the experiment was run. If false, the git state was clean\", \"title\": \"Dirty\"}}, \"required\": [\"commit\", \"branch\", \"tag\", \"commit_time\", \"author_name\", \"author_email\", \"commit_message\", \"dirty\"], \"title\": \"ExperimentGitState\", \"type\": \"object\"}}, \"properties\": {\"id\": {\"description\": \"Experiment ID, unique\", \"title\": \"Id\", \"type\": \"string\"}, \"name\": {\"description\": \"Name of the experiment\", \"title\": \"Name\", \"type\": \"string\"}, \"last_updated\": {\"description\": \"Timestamp marking when the experiment was last updated. If the query deals with some notion of relative time, like age or recency, refer to this timestamp and, if appropriate, compare it to the current time `get_current_time()` by adding or subtracting an interval.\", \"title\": \"Last Updated\", \"type\": \"integer\"}, \"creator\": {\"additionalProperties\": {\"type\": \"string\"}, \"description\": \"Information about the experiment creator\", \"title\": \"Creator\", \"type\": \"object\"}, \"source\": {\"allOf\": [{\"$ref\": \"#/$defs/ExperimentGitState\"}], \"description\": \"Git state that the experiment was run on\"}, \"metadata\": {\"description\": \"Custom metadata provided by the user. Ignore this field unless the query mentions metadata or refers to a metadata key specifically\", \"title\": \"Metadata\", \"type\": \"object\"}, \"avg_sql_score\": {\"anyOf\": [{\"type\": \"number\"}, {\"type\": \"null\"}], \"title\": \"Avg Sql Score\"}, \"avg_factuality_score\": {\"anyOf\": [{\"type\": \"number\"}, {\"type\": \"null\"}], \"title\": \"Avg Factuality Score\"}}, \"required\": [\"id\", \"name\", \"last_updated\", \"creator\", \"source\", \"metadata\", \"avg_sql_score\", \"avg_factuality_score\"], \"title\": \"Experiment\", \"type\": \"object\"}\\n</Schema>'},\n",
       "  {'role': 'user', 'content': 'Query: asdf'},\n",
       "  {'role': 'assistant',\n",
       "   'function_call': {'name': 'QUERY',\n",
       "    'arguments': '{\"value\": {\"type\": \"MATCH\", \"explanation\": \"The query has no semantic meaning, so I interpret it as a simple substring match and fall back to the MATCH function.\"}}'}}],\n",
       " 'functions': [{'name': 'QUERY',\n",
       "   'description': 'Break down the query either into a MATCH or SQL call',\n",
       "   'parameters': {'$defs': {'Match': {'properties': {'type': {'const': 'MATCH',\n",
       "        'default': 'MATCH',\n",
       "        'title': 'Type'},\n",
       "       'explanation': {'description': 'Explanation of why I called the MATCH function',\n",
       "        'title': 'Explanation',\n",
       "        'type': 'string'}},\n",
       "      'required': ['explanation'],\n",
       "      'title': 'Match',\n",
       "      'type': 'object'},\n",
       "     'SQL': {'properties': {'type': {'const': 'SQL',\n",
       "        'default': 'SQL',\n",
       "        'title': 'Type'},\n",
       "       'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "        'description': 'SQL filter clause',\n",
       "        'title': 'Filter'},\n",
       "       'sort': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "        'description': 'SQL sort clause',\n",
       "        'title': 'Sort'},\n",
       "       'explanation': {'description': 'Explanation of why I called the SQL function and how I chose the filter and/or sort clauses',\n",
       "        'title': 'Explanation',\n",
       "        'type': 'string'}},\n",
       "      'required': ['filter', 'sort', 'explanation'],\n",
       "      'title': 'SQL',\n",
       "      'type': 'object'}},\n",
       "    'properties': {'value': {'anyOf': [{'$ref': '#/$defs/Match'},\n",
       "       {'$ref': '#/$defs/SQL'}],\n",
       "      'title': 'Value'}},\n",
       "    'required': ['value'],\n",
       "    'title': 'Query',\n",
       "    'type': 'object'}}]}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_expected_messages(query, expected, prompt, score_fields):\n",
    "    args = build_completion_kwargs(\n",
    "        query=first[\"input\"],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        prompt=fine_tune_prompt,\n",
    "        score_fields=score_fields,\n",
    "    )\n",
    "    function_call = transform_function_call(expected)\n",
    "    return {\n",
    "        \"messages\": args[\"messages\"]\n",
    "        + [{\"role\": \"assistant\", \"function_call\": function_call}],\n",
    "        \"functions\": args[\"functions\"],\n",
    "    }\n",
    "\n",
    "\n",
    "build_expected_messages(\n",
    "    first[\"input\"], first[\"expected\"], fine_tune_prompt, SCORE_FIELDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct messages from our train split and few-shot examples, and then fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': 'Table: experiments\\n\\n<Schema>\\n{\"$defs\": {\"ExperimentGitState\": {\"properties\": {\"commit\": {\"description\": \"Git commit hash. Any prefix of this hash at least 7 characters long should be considered an exact match, so use a substring filter rather than string equality to check the commit, e.g. `(source->>\\'commit\\') ILIKE \\'{COMMIT}%\\'`\", \"title\": \"Commit\", \"type\": \"string\"}, \"branch\": {\"description\": \"Git branch name\", \"title\": \"Branch\", \"type\": \"string\"}, \"tag\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"description\": \"Git commit tag\", \"title\": \"Tag\"}, \"commit_time\": {\"description\": \"Git commit timestamp\", \"title\": \"Commit Time\", \"type\": \"integer\"}, \"author_name\": {\"description\": \"Author of git commit\", \"title\": \"Author Name\", \"type\": \"string\"}, \"author_email\": {\"description\": \"Email address of git commit author\", \"title\": \"Author Email\", \"type\": \"string\"}, \"commit_message\": {\"description\": \"Git commit message\", \"title\": \"Commit Message\", \"type\": \"string\"}, \"dirty\": {\"anyOf\": [{\"type\": \"boolean\"}, {\"type\": \"null\"}], \"description\": \"Whether the git state was dirty when the experiment was run. If false, the git state was clean\", \"title\": \"Dirty\"}}, \"required\": [\"commit\", \"branch\", \"tag\", \"commit_time\", \"author_name\", \"author_email\", \"commit_message\", \"dirty\"], \"title\": \"ExperimentGitState\", \"type\": \"object\"}}, \"properties\": {\"id\": {\"description\": \"Experiment ID, unique\", \"title\": \"Id\", \"type\": \"string\"}, \"name\": {\"description\": \"Name of the experiment\", \"title\": \"Name\", \"type\": \"string\"}, \"last_updated\": {\"description\": \"Timestamp marking when the experiment was last updated. If the query deals with some notion of relative time, like age or recency, refer to this timestamp and, if appropriate, compare it to the current time `get_current_time()` by adding or subtracting an interval.\", \"title\": \"Last Updated\", \"type\": \"integer\"}, \"creator\": {\"additionalProperties\": {\"type\": \"string\"}, \"description\": \"Information about the experiment creator\", \"title\": \"Creator\", \"type\": \"object\"}, \"source\": {\"allOf\": [{\"$ref\": \"#/$defs/ExperimentGitState\"}], \"description\": \"Git state that the experiment was run on\"}, \"metadata\": {\"description\": \"Custom metadata provided by the user. Ignore this field unless the query mentions metadata or refers to a metadata key specifically\", \"title\": \"Metadata\", \"type\": \"object\"}, \"avg_sql_score\": {\"anyOf\": [{\"type\": \"number\"}, {\"type\": \"null\"}], \"title\": \"Avg Sql Score\"}, \"avg_factuality_score\": {\"anyOf\": [{\"type\": \"number\"}, {\"type\": \"null\"}], \"title\": \"Avg Factuality Score\"}}, \"required\": [\"id\", \"name\", \"last_updated\", \"creator\", \"source\", \"metadata\", \"avg_sql_score\", \"avg_factuality_score\"], \"title\": \"Experiment\", \"type\": \"object\"}\\n</Schema>'},\n",
       "  {'role': 'user', 'content': 'Query: asdf'},\n",
       "  {'role': 'assistant',\n",
       "   'function_call': {'name': 'QUERY',\n",
       "    'arguments': '{\"value\": {\"type\": \"SQL\", \"filter\": \"(metadata->\\'env\\'->>\\'veggies\\') ILIKE \\'%kale%\\' OR (metadata->\\'env\\'->>\\'veggies\\') ILIKE \\'%spinach%\\'\"}}'}}],\n",
       " 'functions': [{'name': 'QUERY',\n",
       "   'description': 'Break down the query either into a MATCH or SQL call',\n",
       "   'parameters': {'$defs': {'Match': {'properties': {'type': {'const': 'MATCH',\n",
       "        'default': 'MATCH',\n",
       "        'title': 'Type'},\n",
       "       'explanation': {'description': 'Explanation of why I called the MATCH function',\n",
       "        'title': 'Explanation',\n",
       "        'type': 'string'}},\n",
       "      'required': ['explanation'],\n",
       "      'title': 'Match',\n",
       "      'type': 'object'},\n",
       "     'SQL': {'properties': {'type': {'const': 'SQL',\n",
       "        'default': 'SQL',\n",
       "        'title': 'Type'},\n",
       "       'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "        'description': 'SQL filter clause',\n",
       "        'title': 'Filter'},\n",
       "       'sort': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "        'description': 'SQL sort clause',\n",
       "        'title': 'Sort'},\n",
       "       'explanation': {'description': 'Explanation of why I called the SQL function and how I chose the filter and/or sort clauses',\n",
       "        'title': 'Explanation',\n",
       "        'type': 'string'}},\n",
       "      'required': ['filter', 'sort', 'explanation'],\n",
       "      'title': 'SQL',\n",
       "      'type': 'object'}},\n",
       "    'properties': {'value': {'anyOf': [{'$ref': '#/$defs/Match'},\n",
       "       {'$ref': '#/$defs/SQL'}],\n",
       "      'title': 'Value'}},\n",
       "    'required': ['value'],\n",
       "    'title': 'Query',\n",
       "    'type': 'object'}}]}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_records = [r for r in records if r[\"metadata\"][\"split\"] == \"train\"] + [\n",
    "    {\"input\": r[\"query\"], \"expected\": r} for r in few_shot_examples\n",
    "]\n",
    "all_expected_messages = [\n",
    "    build_expected_messages(r[\"input\"], r[\"expected\"], fine_tune_prompt, SCORE_FIELDS)\n",
    "    for r in train_records\n",
    "]\n",
    "\n",
    "print(len(all_expected_messages))\n",
    "all_expected_messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Use the direct OpenAI client, not a proxy\n",
    "sync_client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\", \"<Your OpenAI API Key>\"),\n",
    "    base_url=\"https://api.openai.com/v1\",\n",
    ")\n",
    "\n",
    "file_string = \"\\n\".join(json.dumps(messages) for messages in all_expected_messages)\n",
    "file = sync_client.files.create(\n",
    "    file=io.BytesIO(file_string.encode()), purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = sync_client.fine_tuning.jobs.create(training_file=file.id, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563s elapsed\tFineTuningJob(id='ftjob-h0MGUfUoF9NgI0W6DdEMXtZN', created_at=1708407645, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-g9DVzuADwPF6yY20DMk2rHLx', result_files=[], status='running', trained_tokens=None, training_file='file-r60EVSZ8wGcsD34oJ0KeaNk9', validation_file=None)le=None)ion_file=None)\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "job_id = job.id\n",
    "while True:\n",
    "    info = sync_client.fine_tuning.jobs.retrieve(job_id)\n",
    "    if info.finished_at is not None:\n",
    "        break\n",
    "    print(f\"{time.time() - start:.0f}s elapsed\", end=\"\\t\")\n",
    "    print(str(info), end=\"\\r\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-h0MGUfUoF9NgI0W6DdEMXtZN', created_at=1708407645, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model='ft:gpt-3.5-turbo-0613:personal::8uDJApGQ', finished_at=1708408215, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-g9DVzuADwPF6yY20DMk2rHLx', result_files=['file-MOztneWGL58LPRSDbnddcb05'], status='succeeded', trained_tokens=127005, training_file='file-r60EVSZ8wGcsD34oJ0KeaNk9', validation_file=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-0613:personal::8uDJApGQ'"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = sync_client.fine_tuning.jobs.retrieve(job_id)\n",
    "fine_tuned_model = info.fine_tuned_model\n",
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'ft:gpt-3.5-turbo-0613:personal::8uDJApGQ', 'messages': [{'role': 'system', 'content': 'Table: experiments\\n\\n<Schema>\\n{\"$defs\": {\"ExperimentGitState\": {\"properties\": {\"commit\": {\"description\": \"Git commit hash. Any prefix of this hash at least 7 characters long should be considered an exact match, so use a substring filter rather than string equality to check the commit, e.g. `(source->>\\'commit\\') ILIKE \\'{COMMIT}%\\'`\", \"title\": \"Commit\", \"type\": \"string\"}, \"branch\": {\"description\": \"Git branch name\", \"title\": \"Branch\", \"type\": \"string\"}, \"tag\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"description\": \"Git commit tag\", \"title\": \"Tag\"}, \"commit_time\": {\"description\": \"Git commit timestamp\", \"title\": \"Commit Time\", \"type\": \"integer\"}, \"author_name\": {\"description\": \"Author of git commit\", \"title\": \"Author Name\", \"type\": \"string\"}, \"author_email\": {\"description\": \"Email address of git commit author\", \"title\": \"Author Email\", \"type\": \"string\"}, \"commit_message\": {\"description\": \"Git commit message\", \"title\": \"Commit Message\", \"type\": \"string\"}, \"dirty\": {\"anyOf\": [{\"type\": \"boolean\"}, {\"type\": \"null\"}], \"description\": \"Whether the git state was dirty when the experiment was run. If false, the git state was clean\", \"title\": \"Dirty\"}}, \"required\": [\"commit\", \"branch\", \"tag\", \"commit_time\", \"author_name\", \"author_email\", \"commit_message\", \"dirty\"], \"title\": \"ExperimentGitState\", \"type\": \"object\"}}, \"properties\": {\"id\": {\"description\": \"Experiment ID, unique\", \"title\": \"Id\", \"type\": \"string\"}, \"name\": {\"description\": \"Name of the experiment\", \"title\": \"Name\", \"type\": \"string\"}, \"last_updated\": {\"description\": \"Timestamp marking when the experiment was last updated. If the query deals with some notion of relative time, like age or recency, refer to this timestamp and, if appropriate, compare it to the current time `get_current_time()` by adding or subtracting an interval.\", \"title\": \"Last Updated\", \"type\": \"integer\"}, \"creator\": {\"additionalProperties\": {\"type\": \"string\"}, \"description\": \"Information about the experiment creator\", \"title\": \"Creator\", \"type\": \"object\"}, \"source\": {\"allOf\": [{\"$ref\": \"#/$defs/ExperimentGitState\"}], \"description\": \"Git state that the experiment was run on\"}, \"metadata\": {\"description\": \"Custom metadata provided by the user. Ignore this field unless the query mentions metadata or refers to a metadata key specifically\", \"title\": \"Metadata\", \"type\": \"object\"}, \"avg_sql_score\": {\"anyOf\": [{\"type\": \"number\"}, {\"type\": \"null\"}], \"title\": \"Avg Sql Score\"}, \"avg_factuality_score\": {\"anyOf\": [{\"type\": \"number\"}, {\"type\": \"null\"}], \"title\": \"Avg Factuality Score\"}}, \"required\": [\"id\", \"name\", \"last_updated\", \"creator\", \"source\", \"metadata\", \"avg_sql_score\", \"avg_factuality_score\"], \"title\": \"Experiment\", \"type\": \"object\"}\\n</Schema>'}, {'role': 'user', 'content': 'Query: asdf'}], 'functions': [{'name': 'QUERY', 'description': 'Break down the query either into a MATCH or SQL call', 'parameters': {'$defs': {'Match': {'properties': {'type': {'const': 'MATCH', 'default': 'MATCH', 'title': 'Type'}, 'explanation': {'description': 'Explanation of why I called the MATCH function', 'title': 'Explanation', 'type': 'string'}}, 'required': ['explanation'], 'title': 'Match', 'type': 'object'}, 'SQL': {'properties': {'type': {'const': 'SQL', 'default': 'SQL', 'title': 'Type'}, 'filter': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'SQL filter clause', 'title': 'Filter'}, 'sort': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'SQL sort clause', 'title': 'Sort'}, 'explanation': {'description': 'Explanation of why I called the SQL function and how I chose the filter and/or sort clauses', 'title': 'Explanation', 'type': 'string'}}, 'required': ['filter', 'sort', 'explanation'], 'title': 'SQL', 'type': 'object'}}, 'properties': {'value': {'anyOf': [{'$ref': '#/$defs/Match'}, {'$ref': '#/$defs/SQL'}], 'title': 'Value'}}, 'required': ['value'], 'title': 'Query', 'type': 'object'}}], 'function_call': {'name': 'QUERY'}}\n",
      "ChatCompletion(id='chatcmpl-8uDJIqe5aOxK9fy25QHgdyTfxZWL7', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"value\": {\"type\": \"SQL\", \"filter\": \"((metadata->\\'pipeline\\'->>\\'run_id\\') = \\'184\\' OR (metadata->\\'pipeline\\'->>\\'run_id\\') ILIKE \\'%184%\\')\"}}', name='QUERY'), tool_calls=None))], created=1708408224, model='ft:gpt-3.5-turbo-0613:personal::8uDJApGQ', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=47, prompt_tokens=795, total_tokens=842))\n",
      "FunctionCallOutput(match=False, filter=\"((metadata->'pipeline'->>'run_id') = '184' OR (metadata->'pipeline'->>'run_id') ILIKE '%184%')\", sort=None, explanation=None, error=None)\n"
     ]
    }
   ],
   "source": [
    "ft_prompt_args = build_completion_kwargs(\n",
    "    query=first[\"input\"],\n",
    "    model=fine_tuned_model,\n",
    "    prompt=fine_tune_prompt,\n",
    "    score_fields=SCORE_FIELDS,\n",
    ")\n",
    "del ft_prompt_args[\"temperature\"]\n",
    "print(ft_prompt_args)\n",
    "output = await client.chat.completions.create(**ft_prompt_args)\n",
    "print(output)\n",
    "print(format_output(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Fine tuned model-00fde0a0 is running at http://localhost:3000/app/braintrustdata.com/p/AI%20Search%20Cookbook/Fine%20tuned%20model-00fde0a0\n",
      "AI Search Cookbook [experiment_name=Fine tuned model] (data): 45it [00:00, 35828.34it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63ccc88478e46c5b5d36cf51cf3b2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AI Search Cookbook [experiment_name=Fine tuned model] (tasks):   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Fine tuned model-00fde0a0 compared to Fine tuned model-cbe529fd:\n",
      "43.33% (+43.33%) 'AutoScorer'      score\t(22 improvements, 0 regressions)\n",
      "77.78% (+77.78%) 'function_choice' score\t(35 improvements, 0 regressions)\n",
      "50.52% (+50.52%) 'SQLScorer'       score\t(28 improvements, 0 regressions)\n",
      "\n",
      "47.12s (+4302.34%) 'duration'\t(1 improvements, 44 regressions)\n",
      "\n",
      "See results for Fine tuned model-00fde0a0 at http://localhost:3000/app/braintrustdata.com/p/AI%20Search%20Cookbook/Fine%20tuned%20model-00fde0a0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mEvaluator AI Search Cookbook [experiment_name=Fine tuned model] failed with 1 error\u001b[0m\n",
      "\u001b[91m  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 596, in run_evaluator_task\n",
      "  |     raise exceptiongroup.ExceptionGroup(\n",
      "  | ExceptionGroup: Found exceptions for the following scorers: SQLScorer (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 570, in run_evaluator_task\n",
      "    |     passing_scorers_and_results.append((name, await p))\n",
      "    |                                               ^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 516, in await_or_run_scorer\n",
      "    |     result = await await_or_run(score, **scorer_args)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/py/src/braintrust/framework.py\", line 487, in await_or_run\n",
      "    |     return await f(*args, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 33, in eval_async\n",
      "    |     return await self._run_eval_async(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/ankur/projects/braintrust/sdk/core/py/src/braintrust_core/score.py\", line 43, in _run_eval_async\n",
      "    |     return self._run_eval_sync(output, expected, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 122, in _run_eval_sync\n",
      "    |     filter_score = score_clause(\n",
      "    |                    ^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 70, in score_clause\n",
      "    |     roundtrip_match = 1 if roundtrip(output) == roundtrip(expected) else 0\n",
      "    |                                                 ^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/var/folders/4t/66zjzrhs3wjf928c7dh270kr0000gn/T/ipykernel_90432/2276735427.py\", line 48, in _roundtrip_filter\n",
      "    |     ).fetchall()[0][0]\n",
      "    |       ^^^^^^^^^^\n",
      "    | duckdb.duckdb.ParserException: Parser Error: Error parsing json: parser: syntax error at or near \")\"\n",
      "    +------------------------------------\u001b[0m\n",
      "\u001b[91mAdd --verbose to see full stack traces.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await run_eval(\"Fine tuned model\", fine_tune_prompt, fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
