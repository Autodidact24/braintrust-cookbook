{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Search Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide demonstrates how we developed Braintrust's AI-powered search bar, harnessing the power of Braintrust's evaluation workflow along the way. If you've used Braintrust before, you may be familiar with the project page, which serves as a home base for collections of eval experiments:\n",
    "\n",
    "![Braintrust Project Page](./assets/project-page-sql.png)\n",
    "\n",
    "To find a particular experiment, you can type filter and sort queries into the search bar, using standard SQL syntax. But SQL can be finicky -- it's very easy to run into syntax errors like single quotes instead of double, incorrect JSON extraction syntax, or typos. Users would prefer to just type in an intuitive search like `experiments run on git commit 2a43fd1` or `score under 0.5` and see a corresponding SQL query appear automatically. Let's achieve this using AI, with assistance from Braintrust's eval framework.\n",
    "\n",
    "We'll start by installing some packages and setting up our OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (0.25.0)\n",
      "Requirement already satisfied: autoevals in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (0.0.46)\n",
      "Requirement already satisfied: braintrust in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (0.0.98)\n",
      "Requirement already satisfied: chevron in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: duckdb in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (0.10.0)\n",
      "Requirement already satisfied: openai in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: pydantic in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from Levenshtein) (3.4.0)\n",
      "Requirement already satisfied: pyyaml in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from autoevals) (6.0.1)\n",
      "Requirement already satisfied: braintrust-core in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from autoevals) (0.0.16)\n",
      "Requirement already satisfied: GitPython in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from braintrust) (3.1.41)\n",
      "Requirement already satisfied: requests in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from braintrust) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from braintrust) (4.66.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: sniffio in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from pydantic) (2.16.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (0.16.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from GitPython->braintrust) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from requests->braintrust) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from requests->braintrust) (1.26.18)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython->braintrust) (5.0.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/austin/code/braintrust/venv/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U Levenshtein autoevals braintrust chevron duckdb openai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BRAINTRUST_API_KEY=\n"
     ]
    }
   ],
   "source": [
    "# Set your API key here\n",
    "%env BRAINTRUST_API_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BRAINTRUST_APP_URL=https://www.braintrustdata.com\n",
      "env: BRAINTRUST_ORG_NAME=braintrustdata.com\n",
      "env: BRAINTRUST_API_KEY=sk-CJjoRo2XsQSFg0DsrE87Hne4OWs2jNytx57FNK8fohZac1zb\n"
     ]
    }
   ],
   "source": [
    "%env BRAINTRUST_APP_URL=https://www.braintrustdata.com\n",
    "%env BRAINTRUST_ORG_NAME=braintrustdata.com\n",
    "# %env BRAINTRUST_API_KEY=sk-d2XPSspSFBpBAtRheuVrZt3QmBV8oxzPDR5WlKJRo0Yst4VQ\n",
    "%env BRAINTRUST_API_KEY=sk-CJjoRo2XsQSFg0DsrE87Hne4OWs2jNytx57FNK8fohZac1zb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import braintrust\n",
    "import openai\n",
    "\n",
    "PROJECT_NAME = \"AI Search Cookbook\"\n",
    "\n",
    "# Your Braintrust API key can be used to access any AI provider via the\n",
    "# Braintrust proxy, once you've added the provider API keys to your account.\n",
    "openai_opts = dict(\n",
    "    base_url=\"https://braintrustproxy.com/v1\",\n",
    "    api_key=os.environ[\"BRAINTRUST_API_KEY\"],\n",
    ")\n",
    "client = braintrust.wrap_openai(openai.AsyncOpenAI(default_headers={\"x-bt-use-cache\": \"always\"}, **openai_opts))\n",
    "dataset = braintrust.init_dataset(PROJECT_NAME, \"AI Search Cookbook\", use_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and render the templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ask GPT to translate a search query, we have to account for multiple output options: (1) a SQL filter, (2) a SQL sort, (3) both of the above, or (4) an unsuccessful translation (e.g. for a nonsensical user input). We'll use [function calling](https://platform.openai.com/docs/guides/function-calling) to robustly handle each distinct scenario, with the following output format:\n",
    "\n",
    "- `match`: Whether or not the model was able to translate the search into a valid SQL filter/sort.\n",
    "- `filter`: A `WHERE` clause.\n",
    "- `sort`: An `ORDER BY` clause.\n",
    "- `explanation`: Explanation for the choices above -- this is useful for debugging and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class FunctionCallOutput:\n",
    "    match: bool = False\n",
    "    filter: Optional[str] = None\n",
    "    sort: Optional[str] = None\n",
    "    explanation: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class Match(BaseModel):\n",
    "    explanation: str = Field(..., description=\"Explanation of why I called the MATCH function\")\n",
    "\n",
    "\n",
    "class SQL(BaseModel):\n",
    "    filter: Optional[str] = Field(..., description=\"SQL filter clause\")\n",
    "    sort: Optional[str] = Field(..., description=\"SQL sort clause\")\n",
    "    explanation: str = Field(\n",
    "        ..., description=\"Explanation of why I called the SQL function and how I chose the filter and/or sort clauses\"\n",
    "    )\n",
    "    \n",
    "\n",
    "def function_choices():\n",
    "    return [\n",
    "        {\n",
    "            \"name\": \"MATCH\",\n",
    "            \"description\": \"Interpret the query as a simple substring match\",\n",
    "            \"parameters\": Match.model_json_schema(),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"SQL\",\n",
    "            \"description\": \"Interpret the query as a SQL filter and/or sort clause\",\n",
    "            \"parameters\": SQL.model_json_schema(),\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompts for evaluation in Braintrust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate two different prompts: a shorter prompt with a brief explanation of the problem statement and description of the experiment schema, and a longer prompt that additionally contains a feed of example cases to guide the model. There's nothing special about either of these prompts, and that's OK -- we can iterate and improve the prompts when we use Braintrust to drill down into the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_PROMPT_FILE = \"./assets/short_prompt.tmpl\"\n",
    "LONG_PROMPT_FILE = \"./assets/long_prompt.tmpl\"\n",
    "\n",
    "with open(SHORT_PROMPT_FILE) as f:\n",
    "    short_prompt = f.read()\n",
    "    \n",
    "with open(LONG_PROMPT_FILE) as f:\n",
    "    long_prompt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One detail worth mentioning: each prompt contains a stub for dynamic insertion of the data schema. This is motivated by the need to handle semantic searches like `more than 40 examples` or `score < 0.5` that don't directly reference a column in the base table. We need to tell the model how the data is structured and what each fields actually _means_. We'll construct a descriptive schema using [pydantic](link) and paste it into each prompt to provide the model with this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chevron\n",
    "from typing import Any, Callable, Dict, List\n",
    "\n",
    "\n",
    "class ExperimentGitState(BaseModel):\n",
    "    commit: str = Field(\n",
    "        ...,\n",
    "        description=\"Git commit hash. Any prefix of this hash at least 7 characters long should be considered an exact match, so use a substring filter rather than string equality to check the commit, e.g. `(source->>'commit') ILIKE '{COMMIT}%'`\",\n",
    "    )\n",
    "    branch: str = Field(..., description=\"Git branch name\")\n",
    "    tag: Optional[str] = Field(..., description=\"Git commit tag\")\n",
    "    commit_time: int = Field(..., description=\"Git commit timestamp\")\n",
    "    author_name: str = Field(..., description=\"Author of git commit\")\n",
    "    author_email: str = Field(..., description=\"Email address of git commit author\")\n",
    "    commit_message: str = Field(..., description=\"Git commit message\")\n",
    "    dirty: Optional[bool] = Field(\n",
    "        ...,\n",
    "        description=\"Whether the git state was dirty when the experiment was run. If false, the git state was clean\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Experiment(BaseModel):\n",
    "    id: str = Field(..., description=\"Experiment ID, unique\")\n",
    "    name: str = Field(..., description=\"Name of the experiment\")\n",
    "    last_updated: int = Field(\n",
    "        ...,\n",
    "        description=\"Timestamp marking when the experiment was last updated. If the query deals with some notion of relative time, like age or recency, refer to this timestamp and, if appropriate, compare it to the current time `get_current_time()` by adding or subtracting an interval.\",\n",
    "    )\n",
    "    creator: Dict[str, str] = Field(..., description=\"Information about the experiment creator\")\n",
    "    source: ExperimentGitState = Field(..., description=\"Git state that the experiment was run on\")\n",
    "    metadata: Dict[str, Any] = Field(\n",
    "        ...,\n",
    "        description=\"Custom metadata provided by the user. Ignore this field unless the query mentions metadata or refers to a metadata key specifically\",\n",
    "    )\n",
    "\n",
    "\n",
    "def build_experiment_schema(score_fields: List[str]):\n",
    "    ExperimentWithScoreFields = create_model(\n",
    "        \"Experiment\",\n",
    "        __base__=Experiment,\n",
    "        **{field: (Optional[float], ...) for field in score_fields},\n",
    "    )\n",
    "    return json.dumps(ExperimentWithScoreFields.model_json_schema())\n",
    "\n",
    "\n",
    "def render_prompt(prompt: str, score_fields: List[str]) -> str:\n",
    "    schema = build_experiment_schema(score_fields)\n",
    "    return chevron.render(prompt.strip(), {\"schema\": schema}, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prompts are ready! Before we run our evals, we just need to load some sample data and define our scoring functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our examples. Each example case contains `input` (the search query) and `expected` (function call output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Example:\n",
    "    input: str\n",
    "    expected: FunctionCallOutput\n",
    "    \n",
    "\n",
    "EXAMPLES_FILE = \"./assets/examples.json\"\n",
    "with open(EXAMPLES_FILE) as f:\n",
    "    examples_json = json.load(f)\n",
    "    \n",
    "templates = [Example(input=e[\"input\"], expected=FunctionCallOutput(**e[\"expected\"])) for e in examples_json[\"examples\"]]\n",
    "\n",
    "# Each example contains a few dynamic fields that depends on the experiments\n",
    "# we're searching over. For simplicity, we'll hard-code these fields here.\n",
    "SCORE_FIELDS = ['avg_sql_score', 'avg_factuality_score']\n",
    "\n",
    "\n",
    "def render_example(example: Example, args: Dict[str, Any]) -> Example:\n",
    "    render_optional = lambda template: chevron.render(template, args, warn=True) if template is not None else None\n",
    "    return Example(\n",
    "        input=render_optional(example.input),\n",
    "        expected=FunctionCallOutput(\n",
    "            match=example.expected.match,\n",
    "            filter=render_optional(example.expected.filter),\n",
    "            sort=render_optional(example.expected.sort),\n",
    "            explanation=render_optional(example.expected.explanation),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "examples = [render_example(t, {\"score_fields\": SCORE_FIELDS}) for t in templates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert our examples into a Braintrust dataset so we can introspect and reuse the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 45 records. Here are the first 2...\n",
      "{'id': '101323f7-da03-4f63-8bb4-e1ef1a80c522', 'span_id': '0375d6ab-998b-4c39-9f32-fd8c65e4e9c1', 'root_span_id': '0375d6ab-998b-4c39-9f32-fd8c65e4e9c1', '_xact_id': 1000192548352904752, 'created': '2024-02-19T03:48:32.415527Z', 'project_id': '61ce386b-1dac-4027-980f-2f3baf32c9f4', 'dataset_id': '8a114409-177e-4b3a-ae70-69b050f668ef', 'input': 'metadata.env.veggies includes kale or spinach', 'expected': {'sort': None, 'error': None, 'match': False, 'filter': \"(metadata->'env'->>'veggies') ILIKE '%kale%' OR (metadata->'env'->>'veggies') ILIKE '%spinach%'\", 'explanation': None}, 'metadata': None}\n",
      "{'id': '12db86bf-f395-4276-bf63-67866f31c3f6', 'span_id': 'fa9e9bac-6b4b-40fd-8fe9-aac4b2fec632', 'root_span_id': 'fa9e9bac-6b4b-40fd-8fe9-aac4b2fec632', '_xact_id': 1000192548352904752, 'created': '2024-02-19T03:48:32.415110Z', 'project_id': '61ce386b-1dac-4027-980f-2f3baf32c9f4', 'dataset_id': '8a114409-177e-4b3a-ae70-69b050f668ef', 'input': 'null metadata.rag', 'expected': {'sort': None, 'error': None, 'match': False, 'filter': \"(metadata->>'rag') IS NULL\", 'explanation': 'The word \"null\" indicates that I should check whether the \"rag\" key on the \"metadata\" JSON column is missing, so I use an IS NULL filter. The query does not indicate a sort order.'}, 'metadata': None}\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    # dataset.insert(**dataclasses.asdict(example))\n",
    "    dataset.insert(input=example.input, expected=example.expected)\n",
    "dataset.flush()\n",
    "\n",
    "records = list(dataset)\n",
    "print(f\"Generated {len(records)} records. Here are the first 2...\")\n",
    "for record in records[:2]:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we score our outputs against the ground truth queries? We can't rely on an exact text match, since there are multiple correct ways to translate a SQL query. Instead, we'll use two approximate scoring methods: (1) `SQLScorer`, which roundtrips each query through `json_serialize_sql` to normalize before attempting a direct comparison, and (2) `AutoScorer`, which delegates the scoring task to `gpt-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from braintrust import current_span, traced\n",
    "from autoevals import Score, Scorer, Sql\n",
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "EXPERIMENTS_TABLE = \"./assets/experiments.parquet\";\n",
    "SUMMARY_TABLE = \"./assets/experiments_summary.parquet\";\n",
    "\n",
    "duckdb.sql(f\"DROP TABLE IF EXISTS experiments; CREATE TABLE experiments AS SELECT * FROM '{EXPERIMENTS_TABLE}'\")\n",
    "duckdb.sql(f\"DROP TABLE IF EXISTS experiments_summary; CREATE TABLE experiments_summary AS SELECT * FROM '{SUMMARY_TABLE}'\")\n",
    "\n",
    "\n",
    "def _test_clause(*, filter=None, sort=None) -> bool:\n",
    "    clause = f\"\"\"\n",
    "        SELECT\n",
    "          experiments.id AS id,\n",
    "          experiments.name,\n",
    "          experiments_summary.last_updated,\n",
    "          experiments.user AS creator,\n",
    "          experiments.repo_info AS source,\n",
    "          experiments_summary.* EXCLUDE (experiment_id, last_updated),\n",
    "        FROM experiments\n",
    "        LEFT JOIN experiments_summary ON experiments.id = experiments_summary.experiment_id\n",
    "        {'WHERE ' + filter if filter else ''}\n",
    "        {'ORDER BY ' + sort if sort else ''}\n",
    "    \"\"\"\n",
    "    current_span().log(metadata=dict(test_clause=clause))\n",
    "    try:\n",
    "        duckdb.sql(clause).fetchall()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _single_quote(s):\n",
    "    return f\"\"\"'{s.replace(\"'\", \"''\")}'\"\"\"\n",
    "\n",
    "\n",
    "def _roundtrip_filter(s):\n",
    "    return duckdb.sql(f\"\"\"\n",
    "        SELECT json_deserialize_sql(json_serialize_sql({_single_quote(f\"SELECT 1 WHERE {s}\")}))\n",
    "    \"\"\").fetchall()[0][0]\n",
    "\n",
    "\n",
    "def _roundtrip_sort(s):\n",
    "    return duckdb.sql(f\"\"\"\n",
    "        SELECT json_deserialize_sql(json_serialize_sql({_single_quote(f\"SELECT 1 ORDER BY {s}\")}))\n",
    "    \"\"\").fetchall()[0][0]\n",
    "\n",
    "\n",
    "def score_clause(\n",
    "    output: Optional[str], expected: Optional[str], roundtrip: Callable[[str], str], test_clause: Callable[[str], bool]\n",
    ") -> float:\n",
    "    exact_match = 1 if output == expected else 0\n",
    "    current_span().log(scores=dict(exact_match=exact_match))\n",
    "    if exact_match:\n",
    "        return 1\n",
    "\n",
    "    roundtrip_match = 1 if roundtrip(output) == roundtrip(expected) else 0\n",
    "    current_span().log(scores=dict(roundtrip_match=roundtrip_match))\n",
    "    if roundtrip_match:\n",
    "        return 1\n",
    "\n",
    "    # If the queries aren't equivalent after roundtripping, it's not immediately clear\n",
    "    # whether they are semantically equivalent. Let's at least check that the generated\n",
    "    # clause is valid SQL by running the `test_clause` function defined above, which\n",
    "    # runs a test query against our sample data.\n",
    "    valid_clause_score = 1 if test_clause(output) else 0\n",
    "    current_span().log(scores=dict(valid_clause=valid_clause_score))\n",
    "    if valid_clause_score == 0:\n",
    "        return 0\n",
    "\n",
    "    max_len = max(len(clause) for clause in [output, expected])\n",
    "    if max_len == 0:\n",
    "        current_span().log(metadata=dict(error=\"Bad example: empty clause\"))\n",
    "        return 0\n",
    "    return 1 - (distance(output, expected) / max_len)\n",
    "\n",
    "    \n",
    "class SQLScorer(Scorer):\n",
    "    \"\"\"SQLScorer uses DuckDB's `json_serialize_sql` function to determine whether\n",
    "    the model's chosen filter/sort clause(s) are equivalent to the expected\n",
    "    outputs. If not, we assign partial credit to each clause depending on\n",
    "    (1) whether the clause is valid SQL, as determined by running it against\n",
    "    the actual data and seeing if it errors, and (2) a distance-wise comparison\n",
    "    to the expected text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _run_eval_sync(\n",
    "        self,\n",
    "        output,\n",
    "        expected=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"SQLScorer requires an expected value\")\n",
    "\n",
    "        name = \"SQLScorer\"\n",
    "        expected = FunctionCallOutput(**expected)\n",
    "        \n",
    "        function_choice_score = 1 if output.match == expected.match else 0\n",
    "        current_span().log(scores=dict(function_choice=function_choice_score))\n",
    "        if function_choice_score == 0:\n",
    "            return Score(name=name, score=0)\n",
    "        if expected.match:\n",
    "            return Score(name=name, score=1)\n",
    "\n",
    "        filter_score = None\n",
    "        if output.filter and expected.filter:\n",
    "            with current_span().start_span(\"SimpleFilter\") as span:\n",
    "                filter_score = score_clause(output.filter, expected.filter, _roundtrip_filter, lambda s: _test_clause(filter=s))\n",
    "        elif output.filter or expected.filter:\n",
    "            filter_score = 0\n",
    "        current_span().log(scores=dict(filter=filter_score))\n",
    "\n",
    "        sort_score = None\n",
    "        if output.sort and expected.sort:\n",
    "            with current_span().start_span(\"SimpleSort\") as span:\n",
    "                sort_score = score_clause(output.sort, expected.sort, _roundtrip_sort, lambda s: _test_clause(sort=s))\n",
    "        elif output.sort or expected.sort:\n",
    "            sort_score = 0\n",
    "        current_span().log(scores=dict(sort=sort_score))\n",
    "\n",
    "        scores = [s for s in [filter_score, sort_score] if s is not None]\n",
    "        if len(scores) == 0:\n",
    "            return Score(name=name, score=0, error=\"Bad example: no filter or sort for SQL function call\")\n",
    "        return Score(name=name, score=sum(scores) / len(scores))\n",
    "\n",
    "\n",
    "@traced(\"auto_score_filter\")\n",
    "def auto_score_filter(openai_opts, **kwargs):\n",
    "    return Sql(**openai_opts)(**kwargs)\n",
    "\n",
    "\n",
    "@traced(\"auto_score_sort\")\n",
    "def auto_score_sort(openai_opts, **kwargs):\n",
    "    return Sql(**openai_opts)(**kwargs)\n",
    "\n",
    "\n",
    "class AutoScorer(Scorer):\n",
    "    \"\"\"AutoScorer uses the `Sql` scorer from the autoevals library to auto-score\n",
    "    the model's chosen filter/sort clause(s) against the expected outputs\n",
    "    using an LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **openai_opts):\n",
    "        self.openai_opts = openai_opts\n",
    "\n",
    "    def _run_eval_sync(\n",
    "        self,\n",
    "        output,\n",
    "        expected=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"AutoScorer requires an expected value\")\n",
    "        input = kwargs.get(\"input\")\n",
    "        if input is None or not isinstance(input, str):\n",
    "            raise ValueError(\"AutoScorer requires an input value of type str\")\n",
    "\n",
    "        name = \"AutoScorer\"\n",
    "        expected = FunctionCallOutput(**expected)\n",
    "\n",
    "        function_choice_score = 1 if output.match == expected.match else 0\n",
    "        current_span().log(scores=dict(function_choice=function_choice_score))\n",
    "        if function_choice_score == 0:\n",
    "            return Score(name=name, score=0)\n",
    "        if expected.match:\n",
    "            return Score(name=name, score=1)\n",
    "\n",
    "        filter_score = None\n",
    "        if output.filter and expected.filter:\n",
    "            result = auto_score_filter(\n",
    "                openai_opts=self.openai_opts,\n",
    "                input=input,\n",
    "                output=output.filter,\n",
    "                expected=expected.filter,\n",
    "            )\n",
    "            filter_score = result.score or 0\n",
    "        elif output.filter or expected.filter:\n",
    "            filter_score = 0\n",
    "        current_span().log(scores=dict(filter=filter_score))\n",
    "\n",
    "        sort_score = None\n",
    "        if output.sort and expected.sort:\n",
    "            result = auto_score_sort(\n",
    "                openai_opts=self.openai_opts,\n",
    "                input=input,\n",
    "                output=output.sort,\n",
    "                expected=expected.sort,\n",
    "            )\n",
    "            sort_score = result.score or 0\n",
    "        elif output.sort or expected.sort:\n",
    "            sort_score = 0\n",
    "        current_span().log(scores=dict(sort=sort_score))\n",
    "\n",
    "        scores = [s for s in [filter_score, sort_score] if s is not None]\n",
    "        if len(scores) == 0:\n",
    "            return Score(name=name, score=0, error=\"Bad example: no filter or sort for SQL function call\")\n",
    "        return Score(name=name, score=sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evals!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Braintrust `Eval` framework to set up our experiments according to the prompts, dataset, and scoring functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_completion_kwargs(\n",
    "    *,\n",
    "    query: str,\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    score_fields: List[str],\n",
    "    **kwargs,\n",
    "):\n",
    "    # Inject the JSON schema into the prompt to assist the model.\n",
    "    schema = build_experiment_schema(score_fields=score_fields)\n",
    "    system_message = chevron.render(prompt.strip(), {\"schema\": schema}, warn=True)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"Query: {query}\"},\n",
    "    ]\n",
    "    tools = [{\"type\": \"function\", \"function\": f} for f in function_choices()]\n",
    "    return dict(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "\n",
    "def format_output(completion):\n",
    "    try:\n",
    "        tool_calls = completion.choices[0].message.tool_calls\n",
    "        if tool_calls is None or len(tool_calls) != 1:\n",
    "            raise Exception(\"No tool call: \", completion.choices[0].message)\n",
    "        tool_call = tool_calls[0]\n",
    "        if tool_call.type != \"function\":\n",
    "            raise Exception(\"Tool call is not a function\")\n",
    "        function_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        return FunctionCallOutput(match=function_name == \"MATCH\", **arguments)\n",
    "    except Exception as e:\n",
    "        return FunctionCallOutput(error=str(e))\n",
    "\n",
    "\n",
    "GRADER = \"gpt-4\"  # Used by AutoScorer to grade the model outputs\n",
    "\n",
    "\n",
    "async def run_eval(experiment_name, prompt, model, score_fields=SCORE_FIELDS):\n",
    "    async def task(input):\n",
    "        completion_kwargs = build_completion_kwargs(\n",
    "            query=input,\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            score_fields=score_fields,\n",
    "        )\n",
    "        return format_output(await client.chat.completions.create(**completion_kwargs))\n",
    "\n",
    "    await braintrust.Eval(\n",
    "        name=PROJECT_NAME,\n",
    "        experiment_name=experiment_name,\n",
    "        data=dataset,\n",
    "        task=task,\n",
    "        scores=[SQLScorer(), AutoScorer(**openai_opts, model=GRADER)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it! We'll use `gpt-3.5-turbo` for both evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "await run_eval(\"Short Prompt 2.0\", short_prompt, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "await run_eval(\"Long Prompt 2.0\", long_prompt, \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results in Braintrust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evals will generate a link to the experiment page. Click into an experiment to view the results!\n",
    "\n",
    "If you've just been following along, you can [check out some sample results here](). Type some searches into the search bar to see AI search in action. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Braintrust Project Page](./assets/project-page-sql.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
