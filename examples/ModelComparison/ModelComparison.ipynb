{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing AI models with Braintrust\n",
    "\n",
    "This tutorial will teach you how to use Braintrust to compare the same prompts across different AI models and parameters to help decide on choosing a model to run your AI apps.\n",
    "\n",
    "Before starting, please make sure that you have a Braintrust account. If you do not, please [sign up](https://www.braintrustdata.com). After this tutorial, feel free to dig deeper by visiting [the docs](http://www.braintrustdata.com/docs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "\n",
    "To see a list of dependencies, you can view the accompanying [package.json](https://github.com/braintrustdata/braintrust-cookbook/tree/main/examples/ModelComparison/package.json) file. Feel free to copy/paste snippets of this code to run in your environment, or use [tslab](https://github.com/yunabe/tslab) to run the tutorial in a Jupyter notebook.\n",
    "\n",
    "## Setting up the data\n",
    "\n",
    "For this example, we will use a small subset of data taken from the [google/boolq](https://huggingface.co/datasets/google/boolq) dataset. If you'd like, you can try datasets and prompts from any of the other [cookbooks](https://www.braintrustdata.com/docs/cookbook/) at Braintrust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    input: \u001b[32m'is there a season 2 of hunted on cinemax'\u001b[39m,\n",
      "    expected: \u001b[32m'false'\u001b[39m\n",
      "  },\n",
      "  {\n",
      "    input: \u001b[32m'would a change in price shift the demand curve'\u001b[39m,\n",
      "    expected: \u001b[32m'false'\u001b[39m\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "// curl -X GET \"https://datasets-server.huggingface.co/rows?dataset=google%2Fboolq&config=default&split=train&offset=600&length=5\" > ./assets/dataset.json\n",
    "import dataset from \"./assets/dataset.json\";\n",
    "\n",
    "// labels these 1-3 so that they will be easier to recognize in the app\n",
    "const prompts = [\n",
    "  \"(1) - true or false\",\n",
    "  \"(2) - Answer using true or false only\",\n",
    "  \"(3) - Answer the following question as accurately as possble with the words 'true' or 'false' in lowercase only. Do not include any other words in the response\",\n",
    "];\n",
    "\n",
    "// extract question/answers from rows into input/expected\n",
    "const evalData = dataset.rows.map(({ row: { question, answer } }) => ({\n",
    "  input: question,\n",
    "  expected: `${answer}`,\n",
    "}));\n",
    "console.log(evalData.slice(0, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running comparison evals across multiple models\n",
    "\n",
    "Let's set up some code to compare these prompts and inputs across 3 different models and different temperature values. For this cookbook we will be using different models from OpenAI, but you can use any [LLM client](https://www.braintrustdata.com/docs/guides/tracing#wrapping-a-custom-llm-client).\n",
    "\n",
    "We'll start by initializing an OpenAI client and wrapping it with some Braintrust instrumentation. `wrapOpenAI`\n",
    "is initially a no-op, but later on when we use Braintrust, it will help us capture helpful debugging information about each model's performance. Then we will set up our simple eval to generate data for each combination of model, prompt and temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { wrapOpenAI } from \"braintrust\";\n",
    "import { OpenAI } from \"openai\";\n",
    "\n",
    "const client = wrapOpenAI(\n",
    "  new OpenAI({\n",
    "    apiKey: process.env.OPENAI_API_KEY || \"Your OpenAI API Key\",\n",
    "  })\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async function callModel(\n",
    "  input: string,\n",
    "  {\n",
    "    model,\n",
    "    temperature,\n",
    "    systemPrompt,\n",
    "  }: { model: string; temperature: number; systemPrompt: string }\n",
    ") {\n",
    "  const response = await client.chat.completions.create({\n",
    "    model: model,\n",
    "    messages: [\n",
    "      {\n",
    "        role: \"system\",\n",
    "        content: systemPrompt,\n",
    "      },\n",
    "      {\n",
    "        role: \"user\",\n",
    "        content: input,\n",
    "      },\n",
    "    ],\n",
    "    temperature,\n",
    "    seed: 123,\n",
    "  });\n",
    "  return response.choices[0].message.content || \"\";\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    model: \u001b[32m'gpt-3.5-turbo'\u001b[39m,\n",
      "    temperature: \u001b[33m0\u001b[39m,\n",
      "    prompt: \u001b[32m'(1) - true or false'\u001b[39m\n",
      "  },\n",
      "  {\n",
      "    model: \u001b[32m'gpt-3.5-turbo'\u001b[39m,\n",
      "    temperature: \u001b[33m0\u001b[39m,\n",
      "    prompt: \u001b[32m'(2) - Answer using true or false only'\u001b[39m\n",
      "  },\n",
      "  {\n",
      "    model: \u001b[32m'gpt-3.5-turbo'\u001b[39m,\n",
      "    temperature: \u001b[33m0\u001b[39m,\n",
      "    prompt: \u001b[32m\"(3) - Answer the following question as accurately as possble with the words 'true' or 'false' in lowercase only. Do not include any other words in the response\"\u001b[39m\n",
      "  },\n",
      "  {\n",
      "    model: \u001b[32m'gpt-3.5-turbo'\u001b[39m,\n",
      "    temperature: \u001b[33m0.1\u001b[39m,\n",
      "    prompt: \u001b[32m'(1) - true or false'\u001b[39m\n",
      "  },\n",
      "  {\n",
      "    model: \u001b[32m'gpt-3.5-turbo'\u001b[39m,\n",
      "    temperature: \u001b[33m0.1\u001b[39m,\n",
      "    prompt: \u001b[32m'(2) - Answer using true or false only'\u001b[39m\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const combinations: { model: string; temperature: number; prompt: string }[] =\n",
    "  [];\n",
    "for (const model of [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"]) {\n",
    "  for (const temperature of [\n",
    "    0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1,\n",
    "  ]) {\n",
    "    for (const prompt of prompts) {\n",
    "      combinations.push({\n",
    "        model,\n",
    "        temperature,\n",
    "        prompt,\n",
    "      });\n",
    "    }\n",
    "  }\n",
    "}\n",
    "console.log(combinations.slice(0, 5));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the functions and data that we have set up to run some evals on Braintrust! We will be using two scorers for this eval:\n",
    "\n",
    "1. A simple exact match scorer that will compare the output from the LLM exactly with the expected value\n",
    "2. A Levenshtein scorer which will calculate the Levenshtein distance\n",
    "\n",
    "We are also adding the model, temperature, and prompt into the metadata so that we can use those fields to help our visualization inside the braintrust app after the evals are finished running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import { Eval } from \"braintrust\";\n",
    "import { Levenshtein } from \"autoevals\";\n",
    "\n",
    "const exactMatch = (args: { input; output; expected? }) => {\n",
    "  return {\n",
    "    name: \"ExactMatch\",\n",
    "    score: args.output === args.expected ? 1 : 0,\n",
    "  };\n",
    "};\n",
    "\n",
    "// feel free to loop differently instead depending on your rate limits\n",
    "for (const { model, temperature, prompt } of combinations) {\n",
    "  await Eval(\"Model comparison\", {\n",
    "    data: () =>\n",
    "      evalData.map(({ input, expected }) => ({\n",
    "        input,\n",
    "        expected,\n",
    "      })),\n",
    "    task: async (input) => {\n",
    "      return await callModel(input, {\n",
    "        model: model,\n",
    "        temperature: temperature,\n",
    "        systemPrompt: prompt,\n",
    "      });\n",
    "    },\n",
    "    scores: [exactMatch, Levenshtein],\n",
    "    metadata: {\n",
    "      model,\n",
    "      temperature,\n",
    "      prompt,\n",
    "    },\n",
    "  });\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  projectName: 'Model comparison',\n",
    "  experimentName: 'main-1715969037',\n",
    "  projectUrl: 'https://www.braintrustdata.com/app/braintrustdata.com/p/Model%20comparison',\n",
    "  experimentUrl: 'https://www.braintrustdata.com/app/braintrustdata.com/p/Model%20comparison/experiments/main-1715969037',\n",
    "  comparisonExperimentName: undefined,\n",
    "  scores: {\n",
    "    ExactMatch: { name: 'ExactMatch', score: 0, improvements: 0, regressions: 0 },\n",
    "    Levenshtein: {\n",
    "      name: 'Levenshtein',\n",
    "      score: 0.5133333333333333,\n",
    "      improvements: 0,\n",
    "      regressions: 0\n",
    "    }\n",
    "  },\n",
    "  metrics: {\n",
    "    duration: {\n",
    "      name: 'duration',\n",
    "      metric: 0.3899999141693115,\n",
    "      unit: 's',\n",
    "      improvements: 0,\n",
    "      regressions: 0\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "\n",
    "Now we have successfully run our Evals! Let's log onto [braintrust.dev](braintrust.dev) and take a look at the results.\n",
    "\n",
    "Click into the newly generated project called `Model comparison`, and check it out! You should notice a few things:\n",
    "\n",
    "![initial-chart](assets/initial-chart.png)\n",
    "\n",
    "- You should see that each line represents a score over time, and each data point represents an experiment that was run.\n",
    "- From the code, we ran 99 experiments (11 temperature values * 3 models * 3 prompts), so one line should consist of 99 dots, each with a different combination of temperature, model, and prompt.\n",
    "- Metadata fields with numeric values are automatically populated as viable X and Y axis values.\n",
    "- Enabling the temperature score on the Y axis also illustrates the order which we ran each experiment via the code.\n",
    "\n",
    "![initial-chart-temperature](assets/initial-chart-temperature.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving in\n",
    "\n",
    "This chart allows us to also group data to allow us to compare experiment runs by model, prompt, and temperature. \n",
    "\n",
    "By selecting the `one color per model`, `one symbol per prompt`, and `X Axis temperature`, we can easily visualize the the data across these dimensions.\n",
    "\n",
    "![grouped-chart](assets/grouped-chart.png)\n",
    "\n",
    "Looking at this view of the data allows us to see that gpt-4 performed better than gpt-4o at lower temperatures!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a few lines of code and a few clicks we were able to find meaningful insights into how different models perform across our data!\n",
    "\n",
    "## Parting thoughts\n",
    "\n",
    "This is just the start of evaluating and improving your AI applications. From here, you should run more experiments with larger datasets, and also try out different prompts! Once you have run another set of experiments, come back to the chart and play with the different views and groupings. You can also add  filtering to filter for experiments with specific scores and metadata to find even more insights.\n",
    "\n",
    "Happy evaluating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
