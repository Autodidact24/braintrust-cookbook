{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS part 1: Retrieval\n",
    "\n",
    "[RAGAS](https://github.com/explodinggradients/ragas) is a popular framework for evaluating Retrieval Augmented Generation (RAG) applications. It's been a longstanding\n",
    "popular request to add these metrics to [`autoevals`](https://github.com/braintrustdata/autoevals), so I thought I'd take the opportunity to audit and port them, and\n",
    "share the process openly. Hopefully it serves as a deeper window into evaluating RAG with RAGAS-style analysis, and also a guide on how to write your own evaluators.\n",
    "\n",
    "We'll use the [Coda Help Desk](https://www.braintrustdata.com/docs/cookbook/CodaHelpDesk) data to benchmark RAGAS evaluators, observe issues, and then tweak and port them\n",
    "into autoevals. Although this cookbook is implemented in Python, the evaluators are now available in both Python and Typescript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U autoevals[scipy] braintrust requests openai lancedb markdownify ragas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating QA pairs\n",
    "\n",
    "To start, let's generate a series of questions and expected answers. The next few blocks of code are copied from the [Coda Help Desk](https://www.braintrustdata.com/docs/cookbook/CodaHelpDesk) cookbook, and simply download some articles and then generate QA pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Starring docs in Coda helps to mark documents of personal '\n",
      "           'importance and organizes them in a section called My Shortcuts.',\n",
      " 'contexts': [\"Not all Coda docs are used in the same way. You'll inevitably \"\n",
      "              \"have a few that you use every week, and some that you'll only \"\n",
      "              'use once. This is where starred docs can help you stay '\n",
      "              'organized.\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              'Starring docs is a great way to mark docs of personal '\n",
      "              'importance. After you star a doc, it will live in a section on '\n",
      "              'your doc list called **[My '\n",
      "              'Shortcuts](https://coda.io/shortcuts)**. All starred docs, even '\n",
      "              'from multiple different workspaces, will live in this section.\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              '\\n'\n",
      "              'Starring docs only saves them to your personal My Shortcuts. It '\n",
      "              'doesn’t affect the view for others in your workspace. If you’re '\n",
      "              'wanting to shortcut docs not just for yourself but also for '\n",
      "              'others in your team or workspace, you’ll [use '\n",
      "              'pinning](https://help.coda.io/en/articles/2865511-starred-pinned-docs) '\n",
      "              'instead.'],\n",
      " 'ground_truth': 'Starring docs in Coda helps to mark documents of personal '\n",
      "                 'importance and organizes them in a section called My '\n",
      "                 'Shortcuts.',\n",
      " 'question': 'What is the purpose of starring documents in Coda?'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    ragas_data_list = json.load(f)\n",
    "\n",
    "ragas_ds = Dataset.from_list(ragas_data_list)\n",
    "pprint(ragas_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselining the RAGAS retrieval metrics\n",
    "\n",
    "RAGAS splits metrics into two buckets: generation and retrieval.\n",
    "\n",
    "![RAGAS framework](https://docs.ragas.io/en/stable/_static/imgs/component-wise-metrics.png)\n",
    "\n",
    "We'll start by working through retrieval metrics:\n",
    "\n",
    "- `context_precision`\n",
    "- `context_relevancy`\n",
    "- `context_recall`\n",
    "- `context_entity_recall`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 80/80 [00:08<00:00,  9.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>context_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of starring documents in C...</td>\n",
       "      <td>Starring docs in Coda helps to mark documents ...</td>\n",
       "      <td>Starring docs in Coda helps to mark documents ...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can starring docs in Coda help you?</td>\n",
       "      <td>Starring docs in Coda helps to mark documents ...</td>\n",
       "      <td>Starring docs in Coda helps to mark documents ...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What happens when you star a doc in Coda?</td>\n",
       "      <td>After you star a doc in Coda, it will appear i...</td>\n",
       "      <td>After you star a doc in Coda, it will appear i...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where do starred docs go after you star them i...</td>\n",
       "      <td>After you star a doc in Coda, it will appear i...</td>\n",
       "      <td>After you star a doc in Coda, it will appear i...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can starred docs from different workspaces be ...</td>\n",
       "      <td>Yes, all starred docs, even from multiple diff...</td>\n",
       "      <td>Yes, all starred docs, even from multiple diff...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Is it possible to find starred docs from vario...</td>\n",
       "      <td>Yes, all starred docs, even from multiple diff...</td>\n",
       "      <td>Yes, all starred docs, even from multiple diff...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Does starring a doc in Coda affect other users...</td>\n",
       "      <td>No, starring docs only saves them to your pers...</td>\n",
       "      <td>No, starring docs only saves them to your pers...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Will other users in your workspace be impacted...</td>\n",
       "      <td>No, starring docs only saves them to your pers...</td>\n",
       "      <td>No, starring docs only saves them to your pers...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What should you use if you want to shortcut a ...</td>\n",
       "      <td>If you want to shortcut docs for the whole tea...</td>\n",
       "      <td>If you want to shortcut docs for the whole tea...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do you create shortcuts for docs that are ...</td>\n",
       "      <td>If you want to shortcut docs for the whole tea...</td>\n",
       "      <td>If you want to shortcut docs for the whole tea...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How often might some Coda docs be used compare...</td>\n",
       "      <td>Some Coda docs are used every week while other...</td>\n",
       "      <td>Some Coda docs are used every week while other...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the frequency of use for Coda docs?</td>\n",
       "      <td>Some Coda docs are used every week while other...</td>\n",
       "      <td>Some Coda docs are used every week while other...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What benefit does starring docs provide in ter...</td>\n",
       "      <td>Starring docs helps you stay organized by mark...</td>\n",
       "      <td>Starring docs helps you stay organized by mark...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How does starring documents help with organiza...</td>\n",
       "      <td>Starring docs helps you stay organized by mark...</td>\n",
       "      <td>Starring docs helps you stay organized by mark...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Where can you find the docs you've starred in ...</td>\n",
       "      <td>The docs you've starred in Coda can be found i...</td>\n",
       "      <td>The docs you've starred in Coda can be found i...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In which section do the starred docs appear in...</td>\n",
       "      <td>The docs you've starred in Coda can be found i...</td>\n",
       "      <td>The docs you've starred in Coda can be found i...</td>\n",
       "      <td>[Not all Coda docs are used in the same way. Y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How can I mark a document as a favorite in the...</td>\n",
       "      <td>To star a doc in the doc list, hover over its ...</td>\n",
       "      <td>To star a doc in the doc list, hover over its ...</td>\n",
       "      <td>[**Star your docs**\\n==================\\n\\n\\nT...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the method to star a doc from the doc ...</td>\n",
       "      <td>To star a doc in the doc list, hover over its ...</td>\n",
       "      <td>To star a doc in the doc list, hover over its ...</td>\n",
       "      <td>[**Star your docs**\\n==================\\n\\n\\nT...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Can I star a document from within the document...</td>\n",
       "      <td>Yes, you can star a doc from within by hoverin...</td>\n",
       "      <td>Yes, you can star a doc from within by hoverin...</td>\n",
       "      <td>[**Star your docs**\\n==================\\n\\n\\nT...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Is it possible to add a doc to my favorites wh...</td>\n",
       "      <td>Yes, you can star a doc from within by hoverin...</td>\n",
       "      <td>Yes, you can star a doc from within by hoverin...</td>\n",
       "      <td>[**Star your docs**\\n==================\\n\\n\\nT...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the purpose of starring documents in C...   \n",
       "1             How can starring docs in Coda help you?   \n",
       "2           What happens when you star a doc in Coda?   \n",
       "3   Where do starred docs go after you star them i...   \n",
       "4   Can starred docs from different workspaces be ...   \n",
       "5   Is it possible to find starred docs from vario...   \n",
       "6   Does starring a doc in Coda affect other users...   \n",
       "7   Will other users in your workspace be impacted...   \n",
       "8   What should you use if you want to shortcut a ...   \n",
       "9   How do you create shortcuts for docs that are ...   \n",
       "10  How often might some Coda docs be used compare...   \n",
       "11        What is the frequency of use for Coda docs?   \n",
       "12  What benefit does starring docs provide in ter...   \n",
       "13  How does starring documents help with organiza...   \n",
       "14  Where can you find the docs you've starred in ...   \n",
       "15  In which section do the starred docs appear in...   \n",
       "16  How can I mark a document as a favorite in the...   \n",
       "17  What is the method to star a doc from the doc ...   \n",
       "18  Can I star a document from within the document...   \n",
       "19  Is it possible to add a doc to my favorites wh...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   Starring docs in Coda helps to mark documents ...   \n",
       "1   Starring docs in Coda helps to mark documents ...   \n",
       "2   After you star a doc in Coda, it will appear i...   \n",
       "3   After you star a doc in Coda, it will appear i...   \n",
       "4   Yes, all starred docs, even from multiple diff...   \n",
       "5   Yes, all starred docs, even from multiple diff...   \n",
       "6   No, starring docs only saves them to your pers...   \n",
       "7   No, starring docs only saves them to your pers...   \n",
       "8   If you want to shortcut docs for the whole tea...   \n",
       "9   If you want to shortcut docs for the whole tea...   \n",
       "10  Some Coda docs are used every week while other...   \n",
       "11  Some Coda docs are used every week while other...   \n",
       "12  Starring docs helps you stay organized by mark...   \n",
       "13  Starring docs helps you stay organized by mark...   \n",
       "14  The docs you've starred in Coda can be found i...   \n",
       "15  The docs you've starred in Coda can be found i...   \n",
       "16  To star a doc in the doc list, hover over its ...   \n",
       "17  To star a doc in the doc list, hover over its ...   \n",
       "18  Yes, you can star a doc from within by hoverin...   \n",
       "19  Yes, you can star a doc from within by hoverin...   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0   Starring docs in Coda helps to mark documents ...   \n",
       "1   Starring docs in Coda helps to mark documents ...   \n",
       "2   After you star a doc in Coda, it will appear i...   \n",
       "3   After you star a doc in Coda, it will appear i...   \n",
       "4   Yes, all starred docs, even from multiple diff...   \n",
       "5   Yes, all starred docs, even from multiple diff...   \n",
       "6   No, starring docs only saves them to your pers...   \n",
       "7   No, starring docs only saves them to your pers...   \n",
       "8   If you want to shortcut docs for the whole tea...   \n",
       "9   If you want to shortcut docs for the whole tea...   \n",
       "10  Some Coda docs are used every week while other...   \n",
       "11  Some Coda docs are used every week while other...   \n",
       "12  Starring docs helps you stay organized by mark...   \n",
       "13  Starring docs helps you stay organized by mark...   \n",
       "14  The docs you've starred in Coda can be found i...   \n",
       "15  The docs you've starred in Coda can be found i...   \n",
       "16  To star a doc in the doc list, hover over its ...   \n",
       "17  To star a doc in the doc list, hover over its ...   \n",
       "18  Yes, you can star a doc from within by hoverin...   \n",
       "19  Yes, you can star a doc from within by hoverin...   \n",
       "\n",
       "                                             contexts  context_precision  \\\n",
       "0   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "1   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "2   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "3   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "4   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "5   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "6   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "7   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "8   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "9   [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "10  [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "11  [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "12  [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "13  [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "14  [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "15  [Not all Coda docs are used in the same way. Y...                1.0   \n",
       "16  [**Star your docs**\\n==================\\n\\n\\nT...                1.0   \n",
       "17  [**Star your docs**\\n==================\\n\\n\\nT...                1.0   \n",
       "18  [**Star your docs**\\n==================\\n\\n\\nT...                1.0   \n",
       "19  [**Star your docs**\\n==================\\n\\n\\nT...                1.0   \n",
       "\n",
       "    context_recall  context_entity_recall  context_relevancy  \n",
       "0             1.00               0.333333           0.555556  \n",
       "1             1.00               0.333333           0.555556  \n",
       "2             1.00               0.500000           0.555556  \n",
       "3             1.00               0.500000           0.222222  \n",
       "4             1.00               0.000000           0.111111  \n",
       "5             1.00               0.000000           0.111111  \n",
       "6             1.00               0.000000           0.222222  \n",
       "7             1.00               0.000000           0.222222  \n",
       "8             1.00               0.250000           0.111111  \n",
       "9             1.00               0.250000           0.333333  \n",
       "10            0.25               1.000000           0.222222  \n",
       "11            1.00               1.000000           0.222222  \n",
       "12            1.00               0.000000           0.444444  \n",
       "13            1.00               0.000000           0.555556  \n",
       "14            1.00               0.500000           0.222222  \n",
       "15            1.00               0.500000           0.222222  \n",
       "16            1.00               0.333333           0.428571  \n",
       "17            1.00               0.333333           0.428571  \n",
       "18            1.00               0.400000           0.285714  \n",
       "19            1.00               0.400000           0.428571  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_entity_recall,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_relevancy,\n",
    ")\n",
    "\n",
    "score = evaluate(\n",
    "    ragas_ds,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        context_entity_recall,\n",
    "        context_relevancy,\n",
    "    ],\n",
    ")\n",
    "score_df = score.to_pandas()\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_precision: 0.9999999998999997\n",
      "context_recall: 0.9625\n",
      "context_entity_recall: 0.3316666648019445\n",
      "context_relevancy: 0.3230158730158731\n"
     ]
    }
   ],
   "source": [
    "for key in [\"context_precision\", \"context_recall\", \"context_entity_recall\", \"context_relevancy\"]:\n",
    "    print(f\"{key}: {score[key].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the implementation\n",
    "\n",
    "Interesting, `context_precision` and `context_recall` seem fairly high, but `context_entity_recall` and `context_relevancy` are low. Let's dig into these by picking example `4`, which has a low score for both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Can starred docs from different workspaces be accessed in one place?',\n",
       " 'answer': 'Yes, all starred docs, even from multiple different workspaces, will live in the My Shortcuts section.',\n",
       " 'ground_truth': 'Yes, all starred docs, even from multiple different workspaces, will live in the My Shortcuts section.',\n",
       " 'contexts': [\"Not all Coda docs are used in the same way. You'll inevitably have a few that you use every week, and some that you'll only use once. This is where starred docs can help you stay organized.\\n\\n\\n\\nStarring docs is a great way to mark docs of personal importance. After you star a doc, it will live in a section on your doc list called **[My Shortcuts](https://coda.io/shortcuts)**. All starred docs, even from multiple different workspaces, will live in this section.\\n\\n\\n\\nStarring docs only saves them to your personal My Shortcuts. It doesn’t affect the view for others in your workspace. If you’re wanting to shortcut docs not just for yourself but also for others in your team or workspace, you’ll [use pinning](https://help.coda.io/en/articles/2865511-starred-pinned-docs) instead.\"]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ragas_data_list[4]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Entity Recall\n",
    "\n",
    "Let's grab the prompt from `context_entity_recall`, which helps extract entities from the ground truth answer the contexts, and see what's going on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a text, extract unique entities without repetition. Ensure you consider different forms or mentions of the same entity as a single entity.\n",
      "\n",
      "The output should be a well-formatted JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output JSON schema:\n",
      "```\n",
      "{\"type\": \"object\", \"properties\": {\"entities\": {\"title\": \"Entities\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"entities\"]}\n",
      "```\n",
      "\n",
      "Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).\n",
      "\n",
      "Examples:\n",
      "\n",
      "text: \"The Eiffel Tower, located in Paris, France, is one of the most iconic landmarks globally.\\n            Millions of visitors are attracted to it each year for its breathtaking views of the city.\\n            Completed in 1889, it was constructed in time for the 1889 World's Fair.\"\n",
      "output: ```{\"entities\": [\"Eiffel Tower\", \"Paris\", \"France\", \"1889\", \"World's Fair\"]}```\n",
      "\n",
      "text: \"The Colosseum in Rome, also known as the Flavian Amphitheatre, stands as a monument to Roman architectural and engineering achievement.\\n            Construction began under Emperor Vespasian in AD 70 and was completed by his son Titus in AD 80.\\n            It could hold between 50,000 and 80,000 spectators who watched gladiatorial contests and public spectacles.\"\n",
      "output: ```{\"entities\": [\"Colosseum\", \"Rome\", \"Flavian Amphitheatre\", \"Vespasian\", \"AD 70\", \"Titus\", \"AD 80\"]}```\n",
      "\n",
      "text: \"The Great Wall of China, stretching over 21,196 kilometers from east to west, is a marvel of ancient defensive architecture.\\n            Built to protect against invasions from the north, its construction started as early as the 7th century BC.\\n            Today, it is a UNESCO World Heritage Site and a major tourist attraction.\"\n",
      "output: ```{\"entities\": [\"Great Wall of China\", \"21,196 kilometers\", \"7th century BC\", \"UNESCO World Heritage Site\"]}```\n",
      "\n",
      "text: \"The Apollo 11 mission, which launched on July 16, 1969, marked the first time humans landed on the Moon.\\n            Astronauts Neil Armstrong, Buzz Aldrin, and Michael Collins made history, with Armstrong being the first man to step on the lunar surface.\\n            This event was a significant milestone in space exploration.\"\n",
      "output: ```{\"entities\": [\"Apollo 11 mission\", \"July 16, 1969\", \"Moon\", \"Neil Armstrong\", \"Buzz Aldrin\", \"Michael Collins\"]}```\n",
      "\n",
      "Your actual task:\n",
      "\n",
      "text: Not all Coda docs are used in the same way. You'll inevitably have a few that you use every week, and some that you'll only use once. This is where starred docs can help you stay organized.\n",
      "\n",
      "\n",
      "\n",
      "Starring docs is a great way to mark docs of personal importance. After you star a doc, it will live in a section on your doc list called **[My Shortcuts](https://coda.io/shortcuts)**. All starred docs, even from multiple different workspaces, will live in this section.\n",
      "\n",
      "\n",
      "\n",
      "Starring docs only saves them to your personal My Shortcuts. It doesn’t affect the view for others in your workspace. If you’re wanting to shortcut docs not just for yourself but also for others in your team or workspace, you’ll [use pinning](https://help.coda.io/en/articles/2865511-starred-pinned-docs) instead.\n",
      "output: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics._context_entities_recall import TEXT_ENTITY_EXTRACTION as CONTEXT_ENTITIES_RECALL_TEMPLATE\n",
    "\n",
    "prompt = CONTEXT_ENTITIES_RECALL_TEMPLATE.format(text=\"\\n\".join(example[\"contexts\"]))\n",
    "print(prompt.prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```{\"entities\": [\"Coda docs\", \"My Shortcuts\", \"workspaces\", \"section\", \"starred docs\", \"view\", \"others\", \"workspace\", \"team\", \"pinning\"]}```\n"
     ]
    }
   ],
   "source": [
    "RAGAS_MODEL = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "resp = await client.chat.completions.create(\n",
    "    model=RAGAS_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt.prompt_str}],\n",
    ")\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```{\"entities\": [\"starred docs\", \"multiple different workspaces\", \"My Shortcuts section\"]}```\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    (\n",
    "        await client.chat.completions.create(\n",
    "            model=RAGAS_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": CONTEXT_ENTITIES_RECALL_TEMPLATE.format(text=example[\"ground_truth\"]).prompt_str,\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "    .choices[0]\n",
    "    .message.content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, only the string `\"starred docs\"` is in both sets, but `multiple different workspaces` and `My Shortcuts section` appears to be covered as well. Let's see if the list comparison in `autoevals` returns\n",
    "a better result.\n",
    "\n",
    "#### Using a better list overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust_core.score import Score, Scorer\n",
    "\n",
    "from autoevals import Levenshtein\n",
    "\n",
    "\n",
    "class ListOverlap(Scorer):\n",
    "    def __init__(self, pairwise_scorer=None, **kwargs):\n",
    "        self.pairwise_scorer = pairwise_scorer or Levenshtein()\n",
    "\n",
    "    async def _run_eval_async(self, output, expected=None, **kwargs):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"ListOverlap requires an expected value\")\n",
    "\n",
    "        distances_futures = [\n",
    "            [self.pairwise_scorer._run_eval_async(output_item, expected_item) for expected_item in expected]\n",
    "            for output_item in output\n",
    "        ]\n",
    "\n",
    "        distances = [\n",
    "            [(await distance_future).score for distance_future in distance_futures]\n",
    "            for distance_futures in distances_futures\n",
    "        ]\n",
    "\n",
    "        return self._compute_scores(output, expected, distances, **kwargs)\n",
    "\n",
    "    def _run_eval_sync(self, output, expected=None, **kwargs):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"ListOverlap requires an expected value\")\n",
    "\n",
    "        distances = [\n",
    "            [self.pairwise_scorer._run_eval_sync(output_item, expected_item).score for expected_item in expected]\n",
    "            for output_item in output\n",
    "        ]\n",
    "\n",
    "        return self._compute_scores(output, expected, distances, **kwargs)\n",
    "\n",
    "    def _compute_scores(self, rows, columns, distances, **kwargs):\n",
    "        import numpy as np\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "        distances = 1 - np.array(distances)\n",
    "        row_ind, col_ind = linear_sum_assignment(distances)\n",
    "\n",
    "        pairs = [(rows[r], columns[c], 1 - distances[r][c]) for (r, c) in zip(row_ind, col_ind)]\n",
    "        lowest_distances = distances[row_ind, col_ind]\n",
    "\n",
    "        # The score is the average of the lowest distances\n",
    "        avg_lowest_distance = lowest_distances.mean()\n",
    "\n",
    "        return Score(\n",
    "            name=self._name(),\n",
    "            score=1 - avg_lowest_distance,\n",
    "            metadata={\"pairs\": pairs, \"lowest_distances\": lowest_distances.tolist()},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(name='ListOverlap',\n",
      "      score=0.8800621780550074,\n",
      "      metadata={'lowest_distances': [0.15267817386741356,\n",
      "                                     0.2071352919675643,\n",
      "                                     0.0],\n",
      "                'pairs': [('My Shortcuts',\n",
      "                           'My Shortcuts section',\n",
      "                           0.8473218261325864),\n",
      "                          ('workspaces',\n",
      "                           'multiple different workspaces',\n",
      "                           0.7928647080324357),\n",
      "                          ('starred docs', 'starred docs', 1.0)]},\n",
      "      error=None)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from autoevals import EmbeddingSimilarity\n",
    "\n",
    "pprint(\n",
    "    await ListOverlap(pairwise_scorer=EmbeddingSimilarity(), allow_extra_entities=True).eval_async(\n",
    "        output=[\n",
    "            \"Coda docs\",\n",
    "            \"My Shortcuts\",\n",
    "            \"workspaces\",\n",
    "            \"section\",\n",
    "            \"starred docs\",\n",
    "            \"view\",\n",
    "            \"others\",\n",
    "            \"workspace\",\n",
    "            \"team\",\n",
    "            \"pinning\",\n",
    "        ],\n",
    "        expected=[\"starred docs\", \"multiple different workspaces\", \"My Shortcuts section\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Relevancy\n",
    "\n",
    "Now let's look at context relevancy. We'll start by examining the prompt used to extract relevant sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
      "\n",
      "Your actual task:\n",
      "\n",
      "question: Can starred docs from different workspaces be accessed in one place?\n",
      "context: Not all Coda docs are used in the same way. You'll inevitably have a few that you use every week, and some that you'll only use once. This is where starred docs can help you stay organized.\n",
      "\n",
      "\n",
      "\n",
      "Starring docs is a great way to mark docs of personal importance. After you star a doc, it will live in a section on your doc list called **[My Shortcuts](https://coda.io/shortcuts)**. All starred docs, even from multiple different workspaces, will live in this section.\n",
      "\n",
      "\n",
      "\n",
      "Starring docs only saves them to your personal My Shortcuts. It doesn’t affect the view for others in your workspace. If you’re wanting to shortcut docs not just for yourself but also for others in your team or workspace, you’ll [use pinning](https://help.coda.io/en/articles/2865511-starred-pinned-docs) instead.\n",
      "candidate sentences: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics._context_relevancy import CONTEXT_RELEVANCE, sent_tokenize\n",
    "\n",
    "prompt = CONTEXT_RELEVANCE.format(question=example[\"question\"], context=\"\\n\".join(example[\"contexts\"]))\n",
    "print(prompt.prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All starred docs, even from multiple different workspaces, will live in this section.\n"
     ]
    }
   ],
   "source": [
    "resp = await client.chat.completions.create(\n",
    "    model=RAGAS_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt.prompt_str}],\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, all starred docs, even from multiple different workspaces, will live in the My Shortcuts section.\n"
     ]
    }
   ],
   "source": [
    "# As a refresher, let's remember the answer\n",
    "print(example[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding chain of thought + function calling\n",
    "\n",
    "Interesting, it appears that at least the previous sentence (\"After you star a doc...My Shortcuts\" is needed to produce the final answer). Let's see if we can improve\n",
    "this metric by asking for Chain of Thought.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RelevantSentence(sentence='After you star a doc, it will live in a section on your doc list called **[My Shortcuts](https://coda.io/shortcuts)**.', reasons=['This sentence directly answers the question by indicating that starred documents are accessible from a designated section, implying that they can be accessed in one place.']),\n",
      " RelevantSentence(sentence='All starred docs, even from multiple different workspaces, will live in this section.', reasons=['This sentence explicitly states that starred documents from different workspaces are available in one section, confirming that they can be accessed in one place.'])]\n"
     ]
    }
   ],
   "source": [
    "class RelevantSentence(BaseModel):\n",
    "    sentence: str = Field(..., description=\"The selected sentence\")\n",
    "    reasons: List[str] = Field(\n",
    "        ..., description=\"Reasons why the sentence is relevant. Explain your thinking step by step.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class RelevantSentences(BaseModel):\n",
    "    sentences: List[RelevantSentence] = Field(..., description=\"List of referenced sentences\")\n",
    "\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=QA_GEN_MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt.prompt_str}],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"extract_sentences\",\n",
    "                \"description\": \"Extract relevant sentences from a given context\",\n",
    "                \"parameters\": RelevantSentences.schema(),\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_sentences\"}},\n",
    ")\n",
    "\n",
    "try:\n",
    "    sentences = RelevantSentences(**json.loads(response.choices[0].message.tool_calls[0].function.arguments))\n",
    "except:\n",
    "    print(\"Failed to parse. Skipping:\")\n",
    "    print(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "pprint(sentences.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting to Autoevals\n",
    "\n",
    "Much better. Let's create an auto evaluator for each of these metrics, and then run an `Eval()` in Braintrust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(name='ContextEntityRecall',\n",
      "      score=0.6952517120038902,\n",
      "      metadata={'context_entities': ['Coda docs',\n",
      "                                     'My Shortcuts',\n",
      "                                     'workspaces',\n",
      "                                     'pinning'],\n",
      "                'expected_entities': ['starred docs',\n",
      "                                      'multiple different workspaces',\n",
      "                                      'My Shortcuts section']},\n",
      "      error=None)\n"
     ]
    }
   ],
   "source": [
    "import chevron\n",
    "\n",
    "from autoevals.list import ListContains\n",
    "from autoevals.llm import OpenAIScorer\n",
    "from autoevals.oai import arun_cached_request, run_cached_request\n",
    "from autoevals.string import EmbeddingSimilarity\n",
    "\n",
    "ENTITY_PROMPT = \"\"\"Given a text, extract unique entities without repetition. Ensure you consider different forms or mentions of the same entity as a single entity.\n",
    "\n",
    "The output should be a well-formatted JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "Here is the output JSON schema:\n",
    "```\n",
    "{\"type\": \"object\", \"properties\": {\"entities\": {\"title\": \"Entities\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"entities\"]}\n",
    "```\n",
    "\n",
    "Do not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).\n",
    "\n",
    "Examples:\n",
    "\n",
    "text: \"The Eiffel Tower, located in Paris, France, is one of the most iconic landmarks globally.\\n            Millions of visitors are attracted to it each year for its breathtaking views of the city.\\n            Completed in 1889, it was constructed in time for the 1889 World's Fair.\"\n",
    "output: ```{\"entities\": [\"Eiffel Tower\", \"Paris\", \"France\", \"1889\", \"World's Fair\"]}```\n",
    "\n",
    "text: \"The Colosseum in Rome, also known as the Flavian Amphitheatre, stands as a monument to Roman architectural and engineering achievement.\\n            Construction began under Emperor Vespasian in AD 70 and was completed by his son Titus in AD 80.\\n            It could hold between 50,000 and 80,000 spectators who watched gladiatorial contests and public spectacles.\"\n",
    "output: ```{\"entities\": [\"Colosseum\", \"Rome\", \"Flavian Amphitheatre\", \"Vespasian\", \"AD 70\", \"Titus\", \"AD 80\"]}```\n",
    "\n",
    "text: \"The Great Wall of China, stretching over 21,196 kilometers from east to west, is a marvel of ancient defensive architecture.\\n            Built to protect against invasions from the north, its construction started as early as the 7th century BC.\\n            Today, it is a UNESCO World Heritage Site and a major tourist attraction.\"\n",
    "output: ```{\"entities\": [\"Great Wall of China\", \"21,196 kilometers\", \"7th century BC\", \"UNESCO World Heritage Site\"]}```\n",
    "\n",
    "Your actual task:\n",
    "\n",
    "text: {{text}}\n",
    "output: \"\"\"\n",
    "\n",
    "# Unfortunately we can't use Pydantic in autoevals because of back-compat issues\n",
    "\n",
    "ENTITY_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\"entities\": {\"title\": \"Entities\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}},\n",
    "    \"required\": [\"entities\"],\n",
    "}\n",
    "\n",
    "\n",
    "def extract_entities_request(text, **extra_args):\n",
    "    return dict(\n",
    "        messages=[{\"role\": \"user\", \"content\": chevron.render(ENTITY_PROMPT, {\"text\": text})}],\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"extract_entities\",\n",
    "                    \"description\": \"Extract unique entities from a given text\",\n",
    "                    \"parameters\": ENTITY_SCHEMA,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_entities\"}},\n",
    "        **extra_args,\n",
    "    )\n",
    "\n",
    "\n",
    "async def aextract_entities(text, **extra_args):\n",
    "    response = await arun_cached_request(**extract_entities_request(text=text, **extra_args))\n",
    "    return json.loads(response[\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
    "\n",
    "\n",
    "def extract_entities(text, **extra_args):\n",
    "    response = run_cached_request(**extract_entities_request(text=text, **extra_args))\n",
    "    return json.loads(response[\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
    "\n",
    "\n",
    "class ContextEntityRecall(OpenAIScorer):\n",
    "    def __init__(self, pairwise_scorer=None, model=\"gpt-3.5-turbo-16k\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.extraction_model = model\n",
    "        self.contains_scorer = ListContains(\n",
    "            pairwise_scorer=pairwise_scorer or EmbeddingSimilarity(), allow_extra_entities=True\n",
    "        )\n",
    "\n",
    "    async def _run_eval_async(self, output, expected=None, context=None, **kwargs):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"ContextEntityRecall requires an expected value\")\n",
    "        if context is None:\n",
    "            raise ValueError(\"ContextEntityRecall requires a context value\")\n",
    "\n",
    "        context = \"\\n\".join(context) if isinstance(context, list) else context\n",
    "\n",
    "        expected_entities = [\n",
    "            e\n",
    "            for e in (await aextract_entities(text=expected, model=self.extraction_model, **self.extra_args))[\n",
    "                \"entities\"\n",
    "            ]\n",
    "        ]\n",
    "        context_entities = [\n",
    "            e\n",
    "            for e in (await aextract_entities(text=context, model=self.extraction_model, **self.extra_args))[\n",
    "                \"entities\"\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        score = await self.contains_scorer.eval_async(output=context_entities, expected=expected_entities)\n",
    "\n",
    "        return Score(\n",
    "            name=self._name(),\n",
    "            score=score.score,\n",
    "            metadata={\"context_entities\": context_entities, \"expected_entities\": expected_entities},\n",
    "        )\n",
    "\n",
    "    def _run_eval_sync(self, output, expected=None, context=None, **kwargs):\n",
    "        if expected is None:\n",
    "            raise ValueError(\"ContextEntityRecall requires an expected value\")\n",
    "        if context is None:\n",
    "            raise ValueError(\"ContextEntityRecall requires a context value\")\n",
    "\n",
    "        context = \"\\n\".join(context) if isinstance(context, list) else context\n",
    "\n",
    "        expected_entities = [\n",
    "            e for e in (extract_entities(text=expected, model=self.extraction_model, **self.extra_args))[\"entities\"]\n",
    "        ]\n",
    "        context_entities = [\n",
    "            e for e in (extract_entities(text=context, model=self.extraction_model, **self.extra_args))[\"entities\"]\n",
    "        ]\n",
    "\n",
    "        score = self.contains_scorer.eval(output=context_entities, expected=expected_entities)\n",
    "\n",
    "        return Score(\n",
    "            name=self._name(),\n",
    "            score=score.score,\n",
    "            metadata={\"context_entities\": context_entities, \"expected_entities\": expected_entities},\n",
    "        )\n",
    "\n",
    "\n",
    "pprint(\n",
    "    await ContextEntityRecall().eval_async(\n",
    "        output=example[\"answer\"], expected=example[\"ground_truth\"], context=example[\"contexts\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$defs': {'RelevantSentence': {'properties': {'sentence': {'description': 'The selected sentence', 'title': 'Sentence', 'type': 'string'}, 'reasons': {'description': 'Reasons why the sentence is relevant. Explain your thinking step by step.', 'items': {'type': 'string'}, 'title': 'Reasons', 'type': 'array'}}, 'required': ['sentence', 'reasons'], 'title': 'RelevantSentence', 'type': 'object'}}, 'properties': {'sentences': {'description': 'List of referenced sentences', 'items': {'$ref': '#/$defs/RelevantSentence'}, 'title': 'Sentences', 'type': 'array'}}, 'required': ['sentences'], 'title': 'RelevantSentences', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# Unfortunately we cannot use pydantic in autoevals due to back-compat issues\n",
    "print(RelevantSentences.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(name='ContextRelevancy',\n",
      "      score=0.7423076923076923,\n",
      "      metadata={'relevant_sentences': [{'reasons': [],\n",
      "                                        'sentence': 'Starring docs is a great '\n",
      "                                                    'way to mark docs of '\n",
      "                                                    'personal importance.'},\n",
      "                                       {'reasons': [],\n",
      "                                        'sentence': 'After you star a doc, it '\n",
      "                                                    'will live in a section on '\n",
      "                                                    'your doc list called '\n",
      "                                                    '**[My '\n",
      "                                                    'Shortcuts](https://coda.io/shortcuts)**.'},\n",
      "                                       {'reasons': [],\n",
      "                                        'sentence': 'All starred docs, even '\n",
      "                                                    'from multiple different '\n",
      "                                                    'workspaces, will live in '\n",
      "                                                    'this section.'},\n",
      "                                       {'reasons': [],\n",
      "                                        'sentence': 'Starring docs only saves '\n",
      "                                                    'them to your personal My '\n",
      "                                                    'Shortcuts.'},\n",
      "                                       {'reasons': [],\n",
      "                                        'sentence': 'It doesn’t affect the '\n",
      "                                                    'view for others in your '\n",
      "                                                    'workspace.'},\n",
      "                                       {'reasons': [],\n",
      "                                        'sentence': 'If you’re wanting to '\n",
      "                                                    'shortcut docs not just '\n",
      "                                                    'for yourself but also for '\n",
      "                                                    'others in your team or '\n",
      "                                                    'workspace, you’ll [use '\n",
      "                                                    'pinning](https://help.coda.io/en/articles/2865511-starred-pinned-docs) '\n",
      "                                                    'instead.'}]},\n",
      "      error=None)\n"
     ]
    }
   ],
   "source": [
    "# Tweaked to return an empty array instead of \"Insufficient information\".\n",
    "SENTENCE_PROMPT = \"\"\"Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return an empty array.  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.\n",
    "\n",
    "Your actual task:\n",
    "\n",
    "question: {{question}}\n",
    "context: {{context}}\n",
    "candidate sentences: \"\"\"\n",
    "\n",
    "SENTENCE_SCHEMA = {\n",
    "    \"$defs\": {\n",
    "        \"RelevantSentence\": {\n",
    "            \"properties\": {\n",
    "                \"sentence\": {\"description\": \"The selected sentence\", \"title\": \"Sentence\", \"type\": \"string\"},\n",
    "                \"reasons\": {\n",
    "                    \"description\": \"Reasons why the sentence is relevant. Explain your thinking step by step.\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"title\": \"Reasons\",\n",
    "                    \"type\": \"array\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"sentence\", \"reasons\"],\n",
    "            \"title\": \"RelevantSentence\",\n",
    "            \"type\": \"object\",\n",
    "        }\n",
    "    },\n",
    "    \"properties\": {\n",
    "        \"sentences\": {\n",
    "            \"description\": \"List of referenced sentences\",\n",
    "            \"items\": {\"$ref\": \"#/$defs/RelevantSentence\"},\n",
    "            \"title\": \"Sentences\",\n",
    "            \"type\": \"array\",\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"sentences\"],\n",
    "    \"title\": \"RelevantSentences\",\n",
    "    \"type\": \"object\",\n",
    "}\n",
    "\n",
    "\n",
    "def extract_sentences_request(question, context, **extra_args):\n",
    "    return dict(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": chevron.render(SENTENCE_PROMPT, {\"question\": question, \"context\": context})}\n",
    "        ],\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"extract_sentences\",\n",
    "                    \"description\": \"Extract relevant sentences from a given context\",\n",
    "                    \"parameters\": SENTENCE_SCHEMA,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_sentences\"}},\n",
    "        **extra_args,\n",
    "    )\n",
    "\n",
    "\n",
    "class ContextRelevancy(OpenAIScorer):\n",
    "    def __init__(self, pairwise_scorer=None, model=\"gpt-3.5-turbo-16k\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    async def _run_eval_async(self, output, expected=None, input=None, context=None, **kwargs):\n",
    "        if input is None:\n",
    "            raise ValueError(\"ContextRelevancy requires an input value\")\n",
    "        if context is None:\n",
    "            raise ValueError(\"ContextRelevancy requires a context value\")\n",
    "\n",
    "        if isinstance(context, list):\n",
    "            context = \"\\n\".join(context)\n",
    "\n",
    "        response = await arun_cached_request(\n",
    "            **extract_sentences_request(question=input, context=context, model=self.model, **self.extra_args)\n",
    "        )\n",
    "        sentences = json.loads(response[\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
    "\n",
    "        return Score(\n",
    "            name=self._name(),\n",
    "            score=len(\"\".join([s[\"sentence\"] for s in sentences[\"sentences\"]])) / len(context),\n",
    "            metadata={\n",
    "                \"relevant_sentences\": sentences[\"sentences\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def _run_eval_sync(self, output, expected=None, input=None, context=None, **kwargs):\n",
    "        if input is None:\n",
    "            raise ValueError(\"ContextRelevancy requires an input value\")\n",
    "        if context is None:\n",
    "            raise ValueError(\"ContextRelevancy requires a context value\")\n",
    "\n",
    "        if isinstance(context, list):\n",
    "            context = \"\\n\".join(context)\n",
    "\n",
    "        response = run_cached_request(\n",
    "            **extract_sentences_request(question=input, context=context, model=self.model, **self.extra_args)\n",
    "        )\n",
    "        sentences = json.loads(response[\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
    "\n",
    "        return Score(\n",
    "            name=self._name(),\n",
    "            score=len(\"\".join([s[\"sentence\"] for s in sentences[\"sentences\"]])) / len(context),\n",
    "            metadata={\n",
    "                \"relevant_sentences\": sentences[\"sentences\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "pprint(\n",
    "    await ContextRelevancy().eval_async(\n",
    "        input=example[\"question\"],\n",
    "        output=example[\"answer\"],\n",
    "        expected=example[\"ground_truth\"],\n",
    "        context=example[\"contexts\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an Eval\n",
    "\n",
    "Now that we have `Scorer`s for each metric, we can easily run an `Eval()` in Braintrust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment ragas-1712521002 is running at http://localhost:3000/app/braintrustdata.com/p/Ragas%20Retrieval/experiments/ragas-1712521002\n",
      "Ragas Retrieval (data): 20it [00:00, 7364.24it/s]\n",
      "Ragas Retrieval (tasks): 100%|██████████| 20/20 [00:07<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "See results for ragas-1712521002 at http://localhost:3000/app/braintrustdata.com/p/Ragas%20Retrieval/experiments/ragas-1712521002\n",
      "\n",
      "=========================SUMMARY=========================\n",
      "See results for ragas-1712521002 at http://localhost:3000/app/braintrustdata.com/p/Ragas%20Retrieval/experiments/ragas-1712521002\n"
     ]
    }
   ],
   "source": [
    "from braintrust import Eval\n",
    "\n",
    "\n",
    "async def context_entites(input, output, expected, metadata):\n",
    "    return await ContextEntityRecall().eval_async(output=output, expected=expected, context=metadata[\"contexts\"])\n",
    "\n",
    "\n",
    "async def context_relevancy(input, output, expected, metadata):\n",
    "    return await ContextRelevancy().eval_async(\n",
    "        input=input, output=output, expected=expected, context=metadata[\"contexts\"]\n",
    "    )\n",
    "\n",
    "\n",
    "result = await Eval(\n",
    "    name=\"Ragas Retrieval\",\n",
    "    data=[\n",
    "        {\n",
    "            \"input\": {\"question\": x[\"question\"], \"ground_truth\": x[\"answer\"]},\n",
    "            \"expected\": x[\"answer\"],\n",
    "            \"metadata\": {\"contexts\": x[\"contexts\"]},\n",
    "        }\n",
    "        for x in ragas_data_list\n",
    "    ],\n",
    "    task=lambda input: input[\"ground_truth\"],\n",
    "    scores=[context_entites, context_relevancy],\n",
    ")\n",
    "\n",
    "print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, the new scores look much higher, especially `ContextEntitiesScorer`, which we'd expect to be close to `1` for this use case.\n",
    "\n",
    "![scores](./assets/new_scores.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further improvements\n",
    "\n",
    "We can also dig into individual examples, and see exactly the set of extracted/overlapping entities. As you can see, certain examples are clearly broken, as \"Eiffel Tower\"\n",
    "is not in the text.\n",
    "\n",
    "![overlapping entities](./assets/extracted_entities.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Braintrust makes it easy to debug these kinds of issues. For example, we can open up the exact prompt that ran, and try to see if using GPT-4 returns better results.\n",
    "\n",
    "![Try fix](./assets/try_fix.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
